{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG)\n",
    "\n",
    "This is based on https://docs.pinecone.io/docs/gen-qa-openai, but using Astra as a Vector Store, in addition to Pinecone. \n",
    "\n",
    "**_Objective_** : Demonstrate how to add domain-specific context to an OpenAI `Completion` call."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "The following blocks get your environment setup for the project."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Module Install\n",
    "The following modules will be referenced and need to be installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -qU \\\n",
    "    openai \\\n",
    "    cassandra-driver \\\n",
    "    \"pinecone-client[grpc]==2.2.1\" \\\n",
    "    datasets==2.12.0 \\\n",
    "    tqdm \\\n",
    "    ipywidgets \\\n",
    "    pyarrow \\\n",
    "    pandas \\\n",
    "    tiktoken"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to External Services\n",
    "You can choose to set environment variables with these values, or provide a path to a `.json` file containing credentials in the format like:\n",
    "\n",
    "```\n",
    "{\n",
    "\"openai_api_key\":\"<your OpenAPI key>\",\n",
    "\"astra_securebundle_path\" : \"/path/to/secure/bundle.zip\",\n",
    "\"astra_client_id\" : \"<your Astra CLIENT_ID>\",\n",
    "\"astra_client_secret\" : \"<your Astra CLIENT_SECRET>\",\n",
    "\"pinecone_api_key\" : \"<your Pinecone API key>\",\n",
    "\"pinecone_env\" : \"<your Pinecone Environment>\"\n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function\n",
    "The following code block is simply a helper file to faciliate setting project variables that should be considered secrets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to a path to your credentials file\n",
    "credentials_file_path = \"./credentials.json\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "def load_credentials(keys):\n",
    "    credentials = {}\n",
    "    if credentials_file_path:\n",
    "        try:\n",
    "            with open(credentials_file_path) as f:\n",
    "                credentials = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"Unable to load credentials file: {e}\")\n",
    "    \n",
    "    for key in keys:\n",
    "        env_key = key.upper()\n",
    "\n",
    "        # Try to get the environment variable\n",
    "        value = os.getenv(env_key)\n",
    "\n",
    "        # If the environment variable is not set, use the credentials file\n",
    "        if value is None:\n",
    "            value = credentials.get(key, None)\n",
    "\n",
    "        # If the value is still None, raise an exception\n",
    "        if value is None:\n",
    "            raise Exception(f\"{env_key} not set\")\n",
    "\n",
    "        globals()[env_key] = value\n",
    "        os.environ[env_key] = value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to OpenAI\n",
    "This project uses OpenAI for example embeddings and LLM functionality. To proceed with the examples, you will need to have an account created with credits available. See https://platform.openai.com/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "load_credentials(['openai_api_key'])\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "if len(openai.Engine.list()['data'])==0:\n",
    "    raise Exception(\"OPENAI_API_KEY invalid, or otherwise unable to connect\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Astra\n",
    "To store and search in Astra, a connection must be established. You must have created a database that has Vector Search enabled. See https://astra.datastax.com/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "\n",
    "load_credentials(['astra_securebundle_path','astra_client_id','astra_client_secret'])\n",
    "\n",
    "cloud_config = {'secure_connect_bundle': ASTRA_SECUREBUNDLE_PATH}\n",
    "auth_provider = PlainTextAuthProvider(ASTRA_CLIENT_ID, ASTRA_CLIENT_SECRET)\n",
    "cluster = Cluster(cloud=cloud_config, auth_provider=auth_provider)\n",
    "session = cluster.connect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Pinecone\n",
    "To store and search in Pinecone, you must have a valid API key. See https://app.pinecone.io/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Phil\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pinecone\\index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WhoAmIResponse(username='626febb', user_label='test-key-2', projectname='899e839')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pinecone\n",
    "load_credentials(['pinecone_api_key','pinecone_env'])\n",
    "\n",
    "pinecone.init(api_key=os.environ['PINECONE_API_KEY'], environment=os.environ['PINECONE_ENV'])\n",
    "pinecone.whoami()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Completions\n",
    "The OpenAI [Completions API](https://platform.openai.com/docs/api-reference/completions) generates a \"completion\" based on a \"prompt\". Following the Pinecone example, we can ask a question which would be commonly known and understood, and expect a good result. The following example uses the `text-davinci-003` model, which is an older model but is widely available and will suit our purposes. Other models are documented [here](https://platform.openai.com/docs/models/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The 12th person on the moon was Harrison Schmitt, and he landed on December 11, 1972.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def complete(prompt):\n",
    "    # query text-davinci-003\n",
    "    res = openai.Completion.create(\n",
    "        engine='text-davinci-003',\n",
    "        prompt=prompt,\n",
    "        temperature=0,\n",
    "        max_tokens=400,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None\n",
    "    )\n",
    "    return res['choices'][0]['text'].strip()\n",
    "\n",
    "complete('who was the 12th person on the moon and when did they land?')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should tell you that Harrison Schmitt landed on the moon on December 11, 1972, which is factually correct. But that is not always the case, and sometimes the Completion model gets confused. Asking the same question from the Pinecone example, \n",
    "> The ideal answer we'd be looking for is \"Multiple Negatives Ranking (MNR) loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If you only have pairs of related sentences, then the best training method to use for sentence transformers is the supervised learning approach. This approach involves providing the model with labeled data, such as pairs of related sentences, and then training the model to learn the relationships between the sentences. This approach is particularly useful when you have limited data, as it allows the model to learn from the data you have provided.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete('Which training method should I use for sentence transformers when I only have pairs of related sentences?')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pinecone example indicates:\n",
    "\n",
    "> One of the common answers I get to this is:\n",
    "```\n",
    "The best training method to use for fine-tuning a pre-trained model with sentence transformers is the Masked Language Model (MLM) training. MLM training involves randomly masking some of the words in a sentence and then training the model to predict the masked words. This helps the model to learn the context of the sentence and better understand the relationships between words.\n",
    "```\n",
    "\n",
    "> This answer seems pretty convincing right? Yet, it's wrong. MLM is typically used in the pretraining step of a transformer model but cannot be used to fine-tune a sentence-transformer, and has nothing to do with having \"pairs of related sentences\".\n",
    "\n",
    "> An alternative answer I recieve is about supervised learning approach being the most suitable. This is completely true, but it's not specific and doesn't answer the question.\n",
    "\n",
    "**Retrieval Augmented Generation** (RAG) can allow us to pass additional context to the Completion engine. The overview of this process is basically:\n",
    "\n",
    "1. Compute an \"embedding\" of the question text\n",
    "2. Search for contextual information that is most similar to the question by doing a \"vector search\"\n",
    "3. Reframe the question to the Completion engine, adding the additional context"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "An \"embedding\" is a mathematical reduction of complex information (in our case, textual information); we can then search these embeddings to find semantically related text. OpenAI provides a number of [embedding models](https://platform.openai.com/docs/guides/embeddings), and the current recommendation is to use `text-embedding-ada-002` which is what we will do. Let's generate an embedding and see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['object', 'data', 'model', 'usage'])\n",
      "4\n",
      "dict_keys(['object', 'index', 'embedding'])\n"
     ]
    }
   ],
   "source": [
    "embed_model = \"text-embedding-ada-002\"\n",
    "\n",
    "limerick_embedding = openai.Embedding.create(\n",
    "    input=[\n",
    "        \"In the world of data, vast and sprawling,\",\n",
    "        \"Vector embeddings are enthralling.\",\n",
    "        \"Words as points in space, they play,\",\n",
    "        \"Mapping meaning in unique array.\"\n",
    "    ], engine=embed_model\n",
    ")\n",
    "\n",
    "print(limerick_embedding.keys()) # expect to see: dict_keys(['object', 'data', 'model', 'usage'])\n",
    "print(len(limerick_embedding['data'])) # expect to see: 4\n",
    "print(limerick_embedding['data'][0].keys()) # expect to see: dict_keys(['object', 'index', 'embedding'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Embedding` API has created 4 embeddings, stored within the `data` field. Each of these is a vector containing 1536 dimensions; this dimensionality is an attribute of the embedding model - embedding models will generate vectors of varying dimensions. As the `text-embedding-ada-002` model normalizes the data, individual vector dimension values should be between -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n",
      "[-0.009303515776991844, -0.00019089288252871484, 0.014775779098272324, -0.01874099299311638, -0.012967320159077644, 0.027073299512267113, -0.008077782578766346, -0.0013513206504285336, -0.027368010953068733, -0.043510179966688156]\n"
     ]
    }
   ],
   "source": [
    "print(len(limerick_embedding['data'][0]['embedding'])) # expect to see: 1536\n",
    "print(limerick_embedding['data'][0]['embedding'][0:10]) # expect to see a list of 10 float-type numbers between -1 and 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual Data\n",
    "Again from the Pinecone example, it discusses a commonly-referenced dataset from the Hugging Face _Datasets_. The particular dataset we will use contains transcribed audio from several ML and tech YouTube channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/Phil/.cache/huggingface/datasets/jamescalam___json/jamescalam--youtube-transcriptions-08d889f6a5386b9b/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'published', 'url', 'video_id', 'channel_id', 'id', 'text', 'start', 'end'],\n",
       "    num_rows: 208619\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset('jamescalam/youtube-transcriptions', split='train')\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expect something like:\n",
    "```\n",
    "Dataset({\n",
    "    features: ['title', 'published', 'url', 'video_id', 'channel_id', 'id', 'text', 'start', 'end'],\n",
    "    num_rows: 208619\n",
    "})\n",
    "```\n",
    "This data contains a number of line-by-line transcriptions; you can see this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 :  Hi, welcome to the video.\n",
      "3.0 :  So this is the fourth video in a Transformers\n",
      "9.36 :  from Scratch mini series.\n",
      "11.56 :  So if you haven't been following along,\n"
     ]
    }
   ],
   "source": [
    "transposed_data = [dict(zip(data[0:4], col)) for col in zip(*data[0:4].values())]\n",
    "for entry in transposed_data:\n",
    "    print(entry['start'],\": \",entry['text'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expect something like:\n",
    "```\n",
    "0.0 :  Hi, welcome to the video.\n",
    "3.0 :  So this is the fourth video in a Transformers\n",
    "9.36 :  from Scratch mini series.\n",
    "11.56 :  So if you haven't been following along,\n",
    "```\n",
    "\n",
    "These short phrases are helpful for video playback (nobody wants to read paragraphs of text when watching a video), but they are less useful if we want to provide additional context to our Completion engine. By merging these snippets of information within a given video, and overlapping adjacent merged snippets, we can create more substantial blocks of context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44de4515a31848ff85dc5d20e44cf92b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/52155 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "context_data = []\n",
    "\n",
    "window = 20  # number of phrases to combine\n",
    "stride = 4  # number of phrases to 'stride' over, used to create overlap\n",
    "\n",
    "for i in tqdm(range(0, len(data), stride)):\n",
    "    i_end = min(len(data)-1, i+window)\n",
    "    if data[i]['title'] != data[i_end]['title']:\n",
    "        # in this case we skip this entry as we have start/end of two videos\n",
    "        continue\n",
    "    text = ' '.join(data[i:i_end]['text'])\n",
    "    # create the new merged dataset\n",
    "    context_data.append({\n",
    "        'start': data[i]['start'],\n",
    "        'end': data[i_end]['end'],\n",
    "        'title': data[i]['title'],\n",
    "        'text': text,\n",
    "        'id': data[i]['id'],\n",
    "        'url': data[i]['url'],\n",
    "        'published': data[i]['published'],\n",
    "        'channel_id': data[i]['channel_id']\n",
    "    })"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas we previously had 208,619 rows, we now have only 48,688 rows, but each of these is an aggregation of up to 20 rows at a time within each video: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48688\n",
      "0 :  Hi, welcome to the video. So this is the fourth video in a Transformers from Scratch mini series. So if you haven't been following along, we've essentially covered what you can see on the screen. So we got some data. We built a tokenizer with it. And then we've set up our input pipeline ready to begin actually training our model, which is what we're going to cover in this video. So let's move over to the code. And we see here that we have essentially everything we've done so far. So we've built our input data, our input pipeline. And we're now at a point where we have a data loader, PyTorch data loader, ready. And we can begin training a model with it. So there are a few things to be aware of. So I mean, first, let's just have a quick look at the structure of our data.\n",
      "1 :  we've essentially covered what you can see on the screen. So we got some data. We built a tokenizer with it. And then we've set up our input pipeline ready to begin actually training our model, which is what we're going to cover in this video. So let's move over to the code. And we see here that we have essentially everything we've done so far. So we've built our input data, our input pipeline. And we're now at a point where we have a data loader, PyTorch data loader, ready. And we can begin training a model with it. So there are a few things to be aware of. So I mean, first, let's just have a quick look at the structure of our data. So when we're training a model for mass language modeling, we need a few tensors. We need three tensors. And this is for training Roberta, by the way, as well.\n",
      "2 :  ready to begin actually training our model, which is what we're going to cover in this video. So let's move over to the code. And we see here that we have essentially everything we've done so far. So we've built our input data, our input pipeline. And we're now at a point where we have a data loader, PyTorch data loader, ready. And we can begin training a model with it. So there are a few things to be aware of. So I mean, first, let's just have a quick look at the structure of our data. So when we're training a model for mass language modeling, we need a few tensors. We need three tensors. And this is for training Roberta, by the way, as well. Same thing with Bert as well. We have our input IDs, attention mask, and our labels. Our input IDs have roughly 15% of their values masked. So we can see that here we have these two tensors.\n",
      "3 :  we've done so far. So we've built our input data, our input pipeline. And we're now at a point where we have a data loader, PyTorch data loader, ready. And we can begin training a model with it. So there are a few things to be aware of. So I mean, first, let's just have a quick look at the structure of our data. So when we're training a model for mass language modeling, we need a few tensors. We need three tensors. And this is for training Roberta, by the way, as well. Same thing with Bert as well. We have our input IDs, attention mask, and our labels. Our input IDs have roughly 15% of their values masked. So we can see that here we have these two tensors. These are the labels. And we have the real tokens in here, the token IDs. And then in our input IDs tensor, we have these being replaced with mask tokens,\n",
      "4 :  And we can begin training a model with it. So there are a few things to be aware of. So I mean, first, let's just have a quick look at the structure of our data. So when we're training a model for mass language modeling, we need a few tensors. We need three tensors. And this is for training Roberta, by the way, as well. Same thing with Bert as well. We have our input IDs, attention mask, and our labels. Our input IDs have roughly 15% of their values masked. So we can see that here we have these two tensors. These are the labels. And we have the real tokens in here, the token IDs. And then in our input IDs tensor, we have these being replaced with mask tokens, the number fours. So that's the structure of our input data. We've created a Torch data set from it and use that to create a Torch data loader.\n"
     ]
    }
   ],
   "source": [
    "print(len(context_data))\n",
    "for i in range(5):\n",
    "    print(i,\": \",context_data[i]['text'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "0 :  Hi, welcome to the video. So this is the fourth video in a Transformers from Scratch mini series. So if you haven't been following along, we've essentially covered what you can see on the screen. So we got some data. We built a tokenizer with it. And then we've set up our input pipeline ready to begin actually training our model, which is what we're going to cover in this video. So let's move over to the code. And we see here that we have essentially everything we've done so far. So we've built our input data, our input pipeline. And we're now at a point where we have a data loader, PyTorch data loader, ready. And we can begin training a model with it. So there are a few things to be aware of. So I mean, first, let's just have a quick look at the structure of our data.\n",
    "1 :  we've essentially covered what you can see on the screen. So we got some data. We built a tokenizer with it. And then we've set up our input pipeline ready to begin actually training our model, which is what we're going to cover in this video. So let's move over to the code. And we see here that we have essentially everything we've done so far. So we've built our input data, our input pipeline. And we're now at a point where we have a data loader, PyTorch data loader, ready. And we can begin training a model with it. So there are a few things to be aware of. So I mean, first, let's just have a quick look at the structure of our data. So when we're training a model for mass language modeling, we need a few tensors. We need three tensors. And this is for training Roberta, by the way, as well.\n",
    "2 :  ready to begin actually training our model, which is what we're going to cover in this video. So let's move over to the code. And we see here that we have essentially everything we've done so far. So we've built our input data, our input pipeline. And we're now at a point where we have a data loader, PyTorch data loader, ready. And we can begin training a model with it. So there are a few things to be aware of. So I mean, first, let's just have a quick look at the structure of our data. So when we're training a model for mass language modeling, we need a few tensors. We need three tensors. And this is for training Roberta, by the way, as well. Same thing with Bert as well. We have our input IDs, attention mask, and our labels. Our input IDs have roughly 15% of their values masked. So we can see that here we have these two tensors.\n",
    "3 :  we've done so far. So we've built our input data, our input pipeline. And we're now at a point where we have a data loader, PyTorch data loader, ready. And we can begin training a model with it. So there are a few things to be aware of. So I mean, first, let's just have a quick look at the structure of our data. So when we're training a model for mass language modeling, we need a few tensors. We need three tensors. And this is for training Roberta, by the way, as well. Same thing with Bert as well. We have our input IDs, attention mask, and our labels. Our input IDs have roughly 15% of their values masked. So we can see that here we have these two tensors. These are the labels. And we have the real tokens in here, the token IDs. And then in our input IDs tensor, we have these being replaced with mask tokens,\n",
    "4 :  And we can begin training a model with it. So there are a few things to be aware of. So I mean, first, let's just have a quick look at the structure of our data. So when we're training a model for mass language modeling, we need a few tensors. We need three tensors. And this is for training Roberta, by the way, as well. Same thing with Bert as well. We have our input IDs, attention mask, and our labels. Our input IDs have roughly 15% of their values masked. So we can see that here we have these two tensors. These are the labels. And we have the real tokens in here, the token IDs. And then in our input IDs tensor, we have these being replaced with mask tokens, the number fours. So that's the structure of our input data. We've created a Torch data set from it and use that to create a Torch data loader.\n",
    "```\n",
    "\n",
    "If you look at the text, you can see the overlap. On entry `0`, we see the first 4 phrases that we printed above, ending in \"So if you haven't been following along,\", followed by \"we've essentially covered...\". This \"we've essentially covered...\" is the beginning of entry `1`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual Embeddings\n",
    "We can compute embeddings for each of our `context_data` entries by making an OpenAI `Embedding.create` call as above; indeed this is how you would normally expect to do this! Because we want to re-use these embeddings to write to both Pinecone and Astra, we will save them in a `.parquet` file and reference them later. \n",
    "\n",
    "You can download pre-generated embeddings, and this is advised to save you time and money. However, you should review the Generate Embeddings subsection to understand how this `.parquet` file has been created, and how embeddings can be efficiently generated.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Pre-Generated Embeddings\n",
    "\n",
    "To save yourself time and money, embeddings have been generated and saved in a `.zip` file available on Google Drive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (uriginal): https://drive.google.com/uc?export=download&id=1EyEqTGHDmv6yWB3mOEW2Q4x-Eo5c_Auo\n",
      "From (redirected): https://drive.google.com/uc?export=download&id=1EyEqTGHDmv6yWB3mOEW2Q4x-Eo5c_Auo&confirm=t&uuid=7b130595-5d95-442f-bf0c-ac95f1a95d7f\n",
      "To: c:\\Users\\Phil\\Documents\\GenAI\\youtube_transcriptions_with_embeddings.parquet.zip\n",
      "100%|██████████| 496M/496M [00:08<00:00, 61.0MB/s] \n"
     ]
    }
   ],
   "source": [
    "parquet_filename = \"./youtube_transcriptions_with_embeddings.parquet\"\n",
    "\n",
    "# Imports\n",
    "import sys\n",
    "!{sys.executable} -m pip install -qU gdown\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "from urllib.parse import urlparse\n",
    "import gdown\n",
    "\n",
    "# Download the pre-generated file from Google Drive\n",
    "url = 'https://drive.google.com/uc?export=download&id=1EyEqTGHDmv6yWB3mOEW2Q4x-Eo5c_Auo'\n",
    "zip_filename = parquet_filename+\".zip\"\n",
    "gdown.download(url, zip_filename, quiet=False)\n",
    "\n",
    "# Use the 'zipfile' module to extract the zip file\n",
    "with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"./\")\n",
    "\n",
    "# Rename the unzipped file\n",
    "extracted_filename = zip_ref.namelist()[0]\n",
    "shutil.move(extracted_filename, parquet_filename)\n",
    "\n",
    "# Remove the zip file\n",
    "os.remove(zip_filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embeddings \n",
    "To generate the embeddings from `context_data` and store them in a `.parquet` file, we can run the following code.\n",
    "\n",
    "**Note:**\n",
    "1. We call the OpenAI API with a `try/except` block; while we have validated this API works, we could see `RateLimitError` and want to handle that, however crudely;\n",
    "2. We are batching the OpenAI requests; this API is highly-parallelised, and the time to compute 100 embeddings is not significantly different than 1 embedding.\n",
    "\n",
    "This code will take something in the region of ≈15 minutes to complete, and when using the `text-embedding-ada-002` model at June 2023 OpenAI prices, it will cost a bit under US$2.00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68df0d6a5b9d4cdda31140f3dff8fde1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/487 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parquet_filename = \"./youtube_transcriptions_with_embeddings.parquet\"\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def get_embeddings(text_list, embed_model):\n",
    "    # try/except handles RateLimitError\n",
    "    done = False\n",
    "    embedding_list = None\n",
    "    while not done:\n",
    "        try:\n",
    "            response = openai.Embedding.create(input=text_list, engine=embed_model)\n",
    "            embedding_list = [data['embedding'] for data in response['data']]\n",
    "            done = True\n",
    "        except Exception as e:\n",
    "            print(f\"Exception occurred: {e}. Retrying in 5 seconds...\")\n",
    "            time.sleep(5)\n",
    "    return embedding_list\n",
    "\n",
    "batch_size=100\n",
    "context_data_batches = [context_data[i:i+batch_size] for i in range(0, len(context_data), batch_size)]\n",
    "writer = None\n",
    "\n",
    "for batch in tqdm(context_data_batches):\n",
    "    text_list = [batch_entry['text'] for batch_entry in batch]\n",
    "    embeddings = get_embeddings(text_list, embed_model)\n",
    "    \n",
    "    for i, batch_entry in enumerate(batch):\n",
    "        batch_entry['embedding'] = embeddings[i]\n",
    "        df = pd.DataFrame([batch_entry])\n",
    "        table = pa.Table.from_pandas(df)\n",
    "        \n",
    "        if writer is None:\n",
    "            writer = pq.ParquetWriter(parquet_filename, table.schema)\n",
    "\n",
    "        writer.write_table(table)\n",
    "\n",
    "if writer is not None:\n",
    "    writer.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviewing Contextual Embeddings\n",
    "These embeddings are in a ≈1 GB `.parquet` file. We can iterate over this file by reading it in batches (saving memory), and show the first 5 entries as a Pandas dataframe. Note the computed embedding is a vector, as it was in the previous examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>published</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>74.12</td>\n",
       "      <td>Training and Testing an Italian BERT - Transfo...</td>\n",
       "      <td>Hi, welcome to the video. So this is the fourt...</td>\n",
       "      <td>35Pdoyi6ZoQ-t0.0</td>\n",
       "      <td>https://youtu.be/35Pdoyi6ZoQ</td>\n",
       "      <td>2021-07-06 13:00:03 UTC</td>\n",
       "      <td>UCv83tO5cePwHMt1952IVVHw</td>\n",
       "      <td>[-0.010392856784164906, -0.01836877129971981, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.84</td>\n",
       "      <td>88.76</td>\n",
       "      <td>Training and Testing an Italian BERT - Transfo...</td>\n",
       "      <td>we've essentially covered what you can see on ...</td>\n",
       "      <td>35Pdoyi6ZoQ-t15.84</td>\n",
       "      <td>https://youtu.be/35Pdoyi6ZoQ</td>\n",
       "      <td>2021-07-06 13:00:03 UTC</td>\n",
       "      <td>UCv83tO5cePwHMt1952IVVHw</td>\n",
       "      <td>[-0.008712108246982098, -0.004878648091107607,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25.76</td>\n",
       "      <td>102.56</td>\n",
       "      <td>Training and Testing an Italian BERT - Transfo...</td>\n",
       "      <td>ready to begin actually training our model, wh...</td>\n",
       "      <td>35Pdoyi6ZoQ-t25.76</td>\n",
       "      <td>https://youtu.be/35Pdoyi6ZoQ</td>\n",
       "      <td>2021-07-06 13:00:03 UTC</td>\n",
       "      <td>UCv83tO5cePwHMt1952IVVHw</td>\n",
       "      <td>[-0.012133925221860409, -0.009328163228929043,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39.56</td>\n",
       "      <td>114.44</td>\n",
       "      <td>Training and Testing an Italian BERT - Transfo...</td>\n",
       "      <td>we've done so far. So we've built our input da...</td>\n",
       "      <td>35Pdoyi6ZoQ-t39.56</td>\n",
       "      <td>https://youtu.be/35Pdoyi6ZoQ</td>\n",
       "      <td>2021-07-06 13:00:03 UTC</td>\n",
       "      <td>UCv83tO5cePwHMt1952IVVHw</td>\n",
       "      <td>[-0.016255807131528854, -0.006291277706623077,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54.04</td>\n",
       "      <td>127.76</td>\n",
       "      <td>Training and Testing an Italian BERT - Transfo...</td>\n",
       "      <td>And we can begin training a model with it. So ...</td>\n",
       "      <td>35Pdoyi6ZoQ-t54.040000000000006</td>\n",
       "      <td>https://youtu.be/35Pdoyi6ZoQ</td>\n",
       "      <td>2021-07-06 13:00:03 UTC</td>\n",
       "      <td>UCv83tO5cePwHMt1952IVVHw</td>\n",
       "      <td>[-0.017424602061510086, 0.0046691871248185635,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   start     end                                              title  \\\n",
       "0   0.00   74.12  Training and Testing an Italian BERT - Transfo...   \n",
       "1  15.84   88.76  Training and Testing an Italian BERT - Transfo...   \n",
       "2  25.76  102.56  Training and Testing an Italian BERT - Transfo...   \n",
       "3  39.56  114.44  Training and Testing an Italian BERT - Transfo...   \n",
       "4  54.04  127.76  Training and Testing an Italian BERT - Transfo...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Hi, welcome to the video. So this is the fourt...   \n",
       "1  we've essentially covered what you can see on ...   \n",
       "2  ready to begin actually training our model, wh...   \n",
       "3  we've done so far. So we've built our input da...   \n",
       "4  And we can begin training a model with it. So ...   \n",
       "\n",
       "                                id                           url  \\\n",
       "0                 35Pdoyi6ZoQ-t0.0  https://youtu.be/35Pdoyi6ZoQ   \n",
       "1               35Pdoyi6ZoQ-t15.84  https://youtu.be/35Pdoyi6ZoQ   \n",
       "2               35Pdoyi6ZoQ-t25.76  https://youtu.be/35Pdoyi6ZoQ   \n",
       "3               35Pdoyi6ZoQ-t39.56  https://youtu.be/35Pdoyi6ZoQ   \n",
       "4  35Pdoyi6ZoQ-t54.040000000000006  https://youtu.be/35Pdoyi6ZoQ   \n",
       "\n",
       "                 published                channel_id  \\\n",
       "0  2021-07-06 13:00:03 UTC  UCv83tO5cePwHMt1952IVVHw   \n",
       "1  2021-07-06 13:00:03 UTC  UCv83tO5cePwHMt1952IVVHw   \n",
       "2  2021-07-06 13:00:03 UTC  UCv83tO5cePwHMt1952IVVHw   \n",
       "3  2021-07-06 13:00:03 UTC  UCv83tO5cePwHMt1952IVVHw   \n",
       "4  2021-07-06 13:00:03 UTC  UCv83tO5cePwHMt1952IVVHw   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-0.010392856784164906, -0.01836877129971981, ...  \n",
       "1  [-0.008712108246982098, -0.004878648091107607,...  \n",
       "2  [-0.012133925221860409, -0.009328163228929043,...  \n",
       "3  [-0.016255807131528854, -0.006291277706623077,...  \n",
       "4  [-0.017424602061510086, 0.0046691871248185635,...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    "parquet_filename = \"./youtube_transcriptions_with_embeddings.parquet\"\n",
    "\n",
    "pfile = pq.ParquetFile(parquet_filename)\n",
    "\n",
    "rows = next(pfile.iter_batches(batch_size = 5)) \n",
    "df = pa.Table.from_batches([rows]).to_pandas() \n",
    "display(df)\n",
    "\n",
    "pfile.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As `display` shows truncated values, we can satisfy ourselves of the complete contextual text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Hi, welcome to the video. So this is the fourth video in a Transformers from Scratch mini series. So if you haven't been following along, we've essentially covered what you can see on the screen. So we got some data. We built a tokenizer with it. And then we've set up our input pipeline ready to begin actually training our model, which is what we're going to cover in this video. So let's move over to the code. And we see here that we have essentially everything we've done so far. So we've built our input data, our input pipeline. And we're now at a point where we have a data loader, PyTorch data loader, ready. And we can begin training a model with it. So there are a few things to be aware of. So I mean, first, let's just have a quick look at the structure of our data.\n",
      "Embedding: [-0.01039286 -0.01836877 -0.00408195 ...  0.00097853 -0.03348809  0.00288187]\n"
     ]
    }
   ],
   "source": [
    "print('Text:', df.loc[0, 'text'])\n",
    "print('Embedding:', str(df.loc[0, 'embedding']).replace('\\n', ''))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freeing Resources\n",
    "As we now have a Parquet file with the relevant information, we can release some memory resources before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_name in ['data','context_data']:\n",
    "    try:\n",
    "        del var_name\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing Vector Embeddings\n",
    "Recall our objective here: we want to take an input question, enhance that with semantically-related domain-specific context, and send that combination to the Completion model. To achieve this, we are going to need to:\n",
    "1. Compute an embedding for each of our `context_data` entries;\n",
    "2. Save these into a vector database (this is our \"long-term memory\");\n",
    "3. Have a means of querying that vector database for close matches to our question.\n",
    "\n",
    "We need to know the number of dimensions of the model, which we'll demonstrate computationally rather than hard-coding. We will then load the Parquet file into a Pandas dataframe.\n",
    "\n",
    "After this has been loaded, we are going to create an \"index\" in Pinecone, and a \"table\" in Astra. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension_count =  1536\n"
     ]
    }
   ],
   "source": [
    "parquet_filename = \"./youtube_transcriptions_with_embeddings.parquet\"\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    "dimension_count = len(limerick_embedding['data'][0]['embedding']) # 1536 for text-embedding-ada-002\n",
    "print(\"dimension_count = \",dimension_count)\n",
    "\n",
    "pfile = pq.ParquetFile(parquet_filename)\n",
    "df = pfile.read().to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pinecone"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Pinecone Index\n",
    "Creating an index in Pinecone can take some time (1-2 minutes), so be a little patient here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 1536,\n",
      " 'index_fullness': 0.4,\n",
      " 'namespaces': {'': {'vector_count': 48688}},\n",
      " 'total_vector_count': 48688}\n"
     ]
    }
   ],
   "source": [
    "pinecone_index_name = \"rag-qa\"\n",
    "\n",
    "if pinecone_index_name not in pinecone.list_indexes():\n",
    "    # if does not exist, create index\n",
    "    pinecone.create_index(\n",
    "        pinecone_index_name,\n",
    "        dimension=dimension_count,\n",
    "        metric='cosine',\n",
    "        metadata_config={'indexed': ['channel_id', 'published']}\n",
    "    )\n",
    "# connect to index\n",
    "pinecone_index = pinecone.GRPCIndex(pinecone_index_name)\n",
    "# view index stats\n",
    "print(pinecone_index.describe_index_stats())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Pinecone Index\n",
    "Load the data into Pinecone 100 records at a time. This will take ≈10 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48688/48688 [10:01<00:00, 80.93it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Define the batch size for Pinecone\n",
    "pinecone_batch_size = 100\n",
    "\n",
    "# Create a list to hold the items to upsert to Pinecone\n",
    "to_upsert = []\n",
    "\n",
    "# Iterate over the DataFrame\n",
    "for idx, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    # Get metadata and embedding\n",
    "    meta = {\n",
    "        'start': row['start'],\n",
    "        'end': row['end'],\n",
    "        'title': row['title'],\n",
    "        'text': row['text'],\n",
    "        'url': row['url'],\n",
    "        'published': row['published'],\n",
    "        'channel_id': row['channel_id']\n",
    "    }\n",
    "    embed = row['embedding']\n",
    "\n",
    "    # Add to the list of items to upsert\n",
    "    to_upsert.append((row['id'], embed, meta))\n",
    "\n",
    "    # Upsert to Pinecone when we've reached the batch size\n",
    "    if (idx + 1) % pinecone_batch_size == 0:\n",
    "        pinecone_index.upsert(vectors=to_upsert)\n",
    "        to_upsert = []  # Reset the list\n",
    "\n",
    "# Upsert any remaining items\n",
    "if to_upsert:\n",
    "    pinecone_index.upsert(vectors=to_upsert)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Astra"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an Astra Table\n",
    "\n",
    "Because we know that the `text-embedding-ada-002` model produces normalized embeddings, we can use the `dot_product` similiarity function, rather than the default `cosine`. This is mathematically equivalent (once normalized!) and significantly faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x1d1ed59b010>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KEYSPACE_NAME = 'vsearch'\n",
    "TABLE_NAME = 'youtube_transcriptions'\n",
    "\n",
    "session.execute(f\"CREATE TABLE IF NOT EXISTS {KEYSPACE_NAME}.{TABLE_NAME} (id text PRIMARY KEY, start float, end float, title text, url text, published text, channel_id text, transcript_text text, embedding VECTOR<FLOAT, {dimension_count}>)\")\n",
    "session.execute(f\"CREATE CUSTOM INDEX IF NOT EXISTS youtube_transcriptions_ann ON {KEYSPACE_NAME}.{TABLE_NAME} (embedding) USING 'StorageAttachedIndex' WITH OPTIONS = {{ 'similarity_function': 'dot_product' }}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Astra Table\n",
    "Whereas Pinecone provides a \"bulk\" interface, Astra (on its own) does not. However, as it is a serverless Cassandra database it is able to absorb a lot of concurrent writes. The following should take about 5 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1fa7529700f4f35bee590fef76c054f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "from cassandra.query import SimpleStatement\n",
    "from tqdm.auto import tqdm\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "class DB:\n",
    "    def __init__(self, cluster: Cluster):\n",
    "        self.session = cluster.connect()\n",
    "\n",
    "    def upsert_one(self, row):\n",
    "        query = SimpleStatement(\n",
    "            f\"\"\"\n",
    "            INSERT INTO {KEYSPACE_NAME}.{TABLE_NAME}\n",
    "            (id, start, end, title, transcript_text, url, published, channel_id, embedding)\n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "            \"\"\"\n",
    "        )\n",
    "        self.session.execute(\n",
    "            query, (\n",
    "                row['id'],\n",
    "                row['start'],\n",
    "                row['end'],\n",
    "                row['title'],\n",
    "                row['text'],\n",
    "                row['url'],\n",
    "                row['published'],\n",
    "                row['channel_id'],\n",
    "                row['embedding']\n",
    "            )\n",
    "        )\n",
    "\n",
    "thread_local_storage = threading.local()\n",
    "\n",
    "def get_db():\n",
    "    if not hasattr(thread_local_storage, 'db_handle'):\n",
    "        thread_local_storage.db_handle = DB(cluster)\n",
    "    return thread_local_storage.db_handle\n",
    "\n",
    "def upsert_row(indexed_row):\n",
    "    _, row = indexed_row  # unpack tuple\n",
    "    db = get_db()\n",
    "    row = row.to_dict()\n",
    "    row['embedding'] = row['embedding'].tolist()\n",
    "    db.upsert_one(row) \n",
    "  \n",
    "num_threads = 64\n",
    "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    list(tqdm(executor.map(upsert_row, df.iterrows()), total=df.shape[0]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving Text With a Vector Embedding\n",
    "We now have a vector database loaded with domain-specific content (the YouTube transcriptions) along with vector embeddings that have been computed by our embedding model. Recall the question we were asking at the very beginning:\n",
    "\n",
    "> Which training method should I use for sentence transformers when I only have pairs of related sentences?\n",
    "\n",
    "We did not get a great answer when we asked for a Completion directly. What do we get if we look in our vector database for a semantic match? We can compute an embedding for our question, and see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.027706358581781387, -0.021540088579058647, 0.03334249556064606, -0.00221120729111135, -0.0021083198953419924, 0.02385592833161354, 0.01801052875816822, -0.006964954547584057, -0.007191655691713095, -0.03931345418095589, 0.012792916037142277, 0.05424084514379501, 0.0022094633895903826, 0.01788496971130371, 0.009256378747522831, 0.01817793771624565, 0.014069417491555214, 0.0035225858446210623, 0.01668519899249077, -0.021930713206529617, -0.019140545278787613, -0.0031964851077646017, -0.00904711615294218, -0.028027227148413658, -0.007868271321058273, -0.012862670235335827, 0.028096981346607208, -0.02333974651992321, -0.02343740314245224, -0.019321907311677933, 0.026673996821045876, 0.009556322358548641, -0.01841510273516178, -0.010756094008684158, 0.01121647097170353, 0.0013898519100621343, 0.015345918945968151, 0.01848485693335533, 0.025376569479703903, -0.018973136320710182, 0.01704792119562626, 0.011321102268993855, 0.006215096917003393, -0.03724873065948486, 0.007679934613406658, 0.011111839674413204, -0.0014325763331726193, -0.01781521551311016, -0.018903382122516632, 0.019196348264813423, 0.003152888733893633, 0.019768333062529564, -0.037220828235149384, -0.007477647624909878, -0.019517218694090843, 0.004010864999145269, 0.00151540944352746, 0.025362618267536163, -0.009263354353606701, -0.005203661508858204, -0.002101344522088766, 0.0003550923429429531, -0.0035243297461420298, 0.013762499205768108, -0.009702805429697037, -0.016406182199716568, -0.011872160248458385, 0.005594284739345312, 0.0029122368432581425, 0.019907841458916664, -0.009012239053845406, 0.022279484197497368, -0.010225961916148663, -0.00596049427986145, 0.026381028816103935, -0.013867130503058434, -0.0015188971301540732, -0.016210870817303658, -0.015276164747774601, 0.009932994842529297, 0.006194170564413071, -0.004788624122738838, -0.010302691720426083, 0.007798516657203436, 0.013044030405580997, 0.002926187589764595, 0.026506586000323296, 0.015750491991639137, -0.0038957707583904266, 0.015499377623200417, 0.016364330425858498, 0.027538947761058807, 0.01870807074010372, 0.018526708707213402, -0.008168214000761509, 0.009702805429697037, -0.00020359500194899738, 0.016503838822245598, 0.005332706496119499, -0.006309265270829201, -0.006281363312155008, 0.009709781035780907, -0.03883912414312363, -0.00728233577683568, -0.009709781035780907, 0.01171172596514225, 0.0031738150864839554, -0.009088968858122826, -0.010714241303503513, -0.00280586164444685, -0.004474730230867863, 0.030887149274349213, 0.008621616289019585, -0.020033398643136024, 0.00299942959100008, -0.02349320612847805, 0.025237061083316803, -0.02399543672800064, -0.01159314438700676, -0.013936884701251984, 0.03451436758041382, -0.02448371611535549, 0.025251012295484543, -0.026018306612968445, 0.04068063572049141, 0.005894227884709835, -0.018652265891432762, -0.019503267481923103, -0.027385490015149117, -0.015778394415974617, 0.0023768735118210316, 0.04729333147406578, 0.025209158658981323, 0.000369261164451018, -0.008391426876187325, 0.014676278457045555, -0.0011666385689750314, -0.03897863253951073, -0.013755523599684238, -0.007219557184726, 0.01505295094102621, -0.005381534807384014, 0.01722928136587143, -0.025530029088258743, -0.0014831481967121363, 0.020437972620129585, 0.008042656816542149, 0.005106005817651749, 0.00899828877300024, -0.030468624085187912, 0.025069650262594223, -0.012897547334432602, -0.005242026410996914, -0.003043025964871049, 0.031556788831949234, 0.002516381908208132, -0.012332538142800331, 0.006490625906735659, -0.00906804297119379, -0.012681309133768082, -0.011872160248458385, 0.01442516315728426, 0.0069754174910485744, -0.006563867907971144, 0.010763069614768028, 0.01703396998345852, 0.020716989412903786, -0.001691538724116981, 0.00906106736510992, -0.04148978367447853, -0.009381936863064766, 0.018652265891432762, -0.013713670894503593, 0.013336998410522938, 0.0063859946094453335, -0.004840939771384001, 0.026743751019239426, -0.02442791312932968, -0.020772794261574745, -0.027148324996232986, 0.004206176847219467, 0.01200469397008419, 0.015011098235845566, 0.0022914246656000614, -0.019642775878310204, -0.021330825984477997, -0.0287387203425169, 0.009605149738490582, 0.005475702695548534, -0.018959185108542442, -0.0005916025838814676, -0.012541800737380981, 0.008251919411122799, -0.009388912469148636, -0.6254438161849976, -0.025613732635974884, 0.0004730205109808594, -0.038002073764801025, 0.0332866907119751, 0.02407914213836193, -0.005182735156267881, 0.012241858057677746, -0.027385490015149117, 0.03655118867754936, 0.006954491138458252, -0.005503604654222727, -0.011404807679355145, 0.019712530076503754, 0.009995773434638977, -0.02946416288614273, 0.013169588521122932, -0.0038190411869436502, 0.026618193835020065, 0.013127735815942287, -0.03998309373855591, 0.027887718752026558, -0.01695026457309723, 0.020242661237716675, -0.01702001877129078, -0.012562726624310017, 0.010616585612297058, -0.01431355718523264, 0.009598174132406712, 0.004907206166535616, -0.02361876331269741, 0.012186054140329361, 0.02325604297220707, -0.009423788636922836, 0.03655118867754936, 0.010246888734400272, 0.0031302187126129866, 0.02378617413341999, 0.005419899243861437, 0.009751633740961552, -0.026785602793097496, -0.007540426217019558, 0.0004935108008794487, 0.012304636649787426, 0.005674502346664667, 0.006539453752338886, 0.009151747450232506, -0.010302691720426083, 0.013176564127206802, -0.00044729866203852, -0.004017840605229139, -0.007407893426716328, -0.011397832073271275, -0.006776618305593729, 0.022209729999303818, 0.007062610238790512, 0.030217507854104042, -0.014397261664271355, 0.003257520031183958, -0.0035051472950726748, 0.02267010696232319, 0.015429623425006866, -0.014801835641264915, -0.019559070467948914, 0.013985712081193924, -0.016141116619110107, -0.015359869226813316, 0.001015795161947608, 0.00021057041885796934, -0.04201991483569145, 0.021191317588090897, 0.004610750824213028, -0.006860323250293732, -0.00371092208661139, -0.01505295094102621, 0.025990406051278114, 0.0061348797753453255, 0.012339513748884201, -0.0036690696142613888, 0.011460610665380955, 0.025111503899097443, -0.013699719682335854, -0.03696971386671066, -0.0067208148539066315, 0.027148324996232986, -0.006856835447251797, -0.020521678030490875, 0.014327507466077805, -0.01456467155367136, -0.01456467155367136, -0.0014264729106798768, 0.03451436758041382, -0.0332866907119751, -0.038141582161188126, -0.0009381936397403479, 0.010030650533735752, 0.0004222307470627129, -0.004418926779180765, 0.026381028816103935, -0.05083684250712395, 0.018945233896374702, 0.005534993950277567, 0.012834767811000347, -0.008342599496245384, -0.003039538161829114, 0.005493141245096922, -0.0040841070003807545, -0.007275360636413097, -0.008140312507748604, 0.018763873726129532, -0.002570441458374262, -0.01165592297911644, 0.006434822455048561, 0.0113629549741745, -0.009772559627890587, -0.038002073764801025, 0.009381936863064766, 0.011669873259961605, 0.002566953655332327, -0.005081591662019491, 0.005555919837206602, 0.010393371805548668, 0.012820817530155182, -0.011830308474600315, -0.010721216909587383, -0.0113629549741745, -0.006026760675013065, 0.006985880434513092, 0.030357016250491142, 0.0076729594729840755, -0.009877190925180912, -0.0025686975568532944, -0.00031694554490968585, -0.013985712081193924, -0.0016278880648314953, 0.015080852434039116, 0.010595658794045448, 0.006016297731548548, 0.00851000938564539, -0.0033412249758839607, -0.026813505217432976, 0.0011317614698782563, 0.0014622218441218138, 0.012527849525213242, -0.030273312702775, -0.01739669218659401, -0.020145006477832794, 0.004875816870480776, -0.003770213108509779, -0.01491344254463911, 0.0012843487784266472, -0.006445285864174366, -0.021623793989419937, -0.013288170099258423, -0.021512188017368317, -0.026074111461639404, -0.010602634400129318, -0.03476548194885254, -0.01717347837984562, -0.04302437603473663, 0.025711389258503914, 0.026785602793097496, -0.013748547993600368, 0.006051174830645323, -0.015513328835368156, -0.010930479504168034, 0.0291572455316782, 0.015680737793445587, 0.010302691720426083, -0.02423259988427162, 0.010581708513200283, -0.05764485150575638, -0.002324557863175869, 0.006392970215529203, 0.006246486213058233, 0.010944429785013199, -0.01794077455997467, 0.004425902385264635, 0.002075186697766185, -0.01781521551311016, -0.02946416288614273, 0.027120422571897507, -0.006239511072635651, 0.007721787318587303, 0.013650892302393913, -0.00463865278288722, -0.007233507931232452, -0.013120760209858418, -0.04241053760051727, 0.03652328625321388, -0.00603722408413887, 0.006229047663509846, 0.0009678392088972032, 0.01170475035905838, -0.0028041177429258823, 0.0299942959100008, 0.015192459337413311, 0.021344777196645737, -0.005869813729077578, 0.01849880814552307, 0.031221969053149223, 0.01143968477845192, 0.02368851751089096, -0.010867700912058353, -0.0015642374055460095, -0.01415312197059393, 0.014048490673303604, -0.015680737793445587, 0.02282356657087803, 0.020647235214710236, 0.009716756641864777, -0.026757702231407166, -0.031863708049058914, -0.03512820228934288, 0.0024239576887339354, 0.01400663796812296, -0.00851000938564539, 0.03657909110188484, -0.011349004693329334, 0.03641168028116226, 0.006365068722516298, 0.0013445117510855198, 0.0355188250541687, 0.013274219818413258, 0.01484368834644556, -0.0033342496026307344, 0.004931620322167873, -0.00700331898406148, 0.032979775220155716, -0.02261430397629738, 0.008168214000761509, 0.009235452860593796, 0.01658754236996174, 0.002268754644319415, 0.00742184417322278, -0.010595658794045448, 0.012569702230393887, 0.01128622516989708, 0.02876662276685238, 0.003281933953985572, 0.008112411014735699, 0.0006474059191532433, -0.004830476827919483, -0.006619671359658241, 0.04542391747236252, -0.00866346899420023, 0.04444735869765282, 0.006790569052100182, -0.010456151328980923, -0.006853347644209862, -0.012193029746413231, 0.004589824937283993, -0.004575873725116253, 0.010965356603264809, -0.000874978955835104, -0.012758038938045502, 0.006229047663509846, -0.008112411014735699, 0.009158723056316376, 0.021442433819174767, 0.019977595657110214, 0.0010532879969105124, 0.015890000388026237, -0.006242998875677586, 0.03546302393078804, 0.006148830521851778, 0.003206948284059763, -0.01805238053202629, -0.003606290789321065, -0.0035609507467597723, 0.0036586064379662275, -0.01204654574394226, 0.010546831414103508, -0.025055699050426483, 0.02336764894425869, 0.006528990808874369, -0.004520070739090443, 0.018666217103600502, 0.013936884701251984, 0.024860387668013573, -0.0088448291644454, -0.02968737669289112, 0.025571880862116814, -0.008182165212929249, -0.020926252007484436, -0.01688051037490368, -0.026771653443574905, -0.0175501499325037, -0.037695154547691345, 0.04082014411687851, 0.02412099391222, 0.010191084817051888, 0.0004503504023887217, 0.0021588916424661875, -0.020186858251690865, 0.010532880201935768, 0.02364666573703289, -0.0022321336437016726, 0.013902007602155209, 0.0038225287571549416, -0.015750491991639137, 0.0022443404886871576, -0.032644953578710556, -0.02346530370414257, 0.027315735816955566, -0.009249404072761536, -0.01477393414825201, -0.037862565368413925, -0.006595257204025984, -0.026199668645858765, 0.00010370484233135357, 0.015555181540548801, -0.006710351444780827, 0.00897736195474863, 0.0037876516580581665, -0.0019199836533516645, -0.0067243026569485664, 4.601459295372479e-06, 0.03133357688784599, 0.032812364399433136, -0.011195545084774494, -0.031500983983278275, 0.005311780609190464, 0.01179543137550354, 0.04249424487352371, 0.05139487609267235, -0.0171316247433424, 0.00046822489821352065, -0.02893403172492981, -0.027873769402503967, -0.0015877793775871396, -0.031584691256284714, 0.021512188017368317, -0.016266673803329468, -0.01848485693335533, -0.010156207717955112, -0.0027483145240694284, -0.00436661159619689, 0.005203661508858204, 0.01743854396045208, -0.007435794919729233, -0.043694015592336655, 0.0021885372698307037, -0.002718668896704912, -0.0018066331977024674, -0.020019447430968285, 0.01703396998345852, 0.031417280435562134, 0.03590944781899452, 0.0025721851270645857, 0.03685810789465904, 0.02410704270005226, 0.019531169906258583, -0.0353793166577816, -0.0007572687463834882, 0.011202520690858364, -0.00207169889472425, 0.00704168388620019, -0.011669873259961605, 0.02816673554480076, 0.002702974248677492, 0.0052490015514194965, 0.01767570711672306, 0.003805090207606554, 0.0033412249758839607, 0.010044600814580917, 0.014829738065600395, -0.007575303316116333, -0.002355947159230709, -0.0172711331397295, 0.0024082628078758717, 0.0055489446967840195, 0.012855694629251957, -0.021233171224594116, -0.020382169634103775, 0.01660149358212948, -0.012499948032200336, -0.00602327287197113, -0.011676848866045475, -0.005126931704580784, -0.020786743611097336, -0.0026733288541436195, -0.024469764903187752, 0.006535966414958239, -0.011628021486103535, -0.013685769401490688, 0.0349607914686203, -0.0021885372698307037, 0.006044199224561453, -0.012848719023168087, -0.008000804111361504, -0.005433850456029177, -0.020214760676026344, -0.01422287616878748, -0.019461415708065033, -0.007812467869371176, -0.021079711616039276, 0.005862838588654995, 0.02353505790233612, 0.01669915020465851, 0.02403728850185871, -0.0030290749855339527, -0.010790971107780933, 0.002526845084503293, -0.014801835641264915, -0.01821979135274887, -0.002504174830392003, -0.005276903510093689, -0.027943521738052368, 0.0223073847591877, -0.049385957419872284, -0.0055070919916033745, -0.006222072523087263, 0.037388239055871964, -0.005629161838442087, 0.02848760597407818, -0.016113214194774628, -0.01491344254463911, 0.01803842931985855, 0.0355188250541687, -0.005733793135732412, 0.01835929974913597, -0.0015912671806290746, -0.01693631336092949, -0.004495656583458185, -0.02854340896010399, 0.005730305332690477, -0.04640047624707222, -0.014327507466077805, 0.005245514214038849, 0.02380012534558773, 0.03518400713801384, 0.0022286458406597376, -7.978134090080857e-05, 0.0037911394611001015, 0.0011265298817306757, -0.0006491498206742108, 0.01449491735547781, 0.026646094396710396, 0.01749434694647789, 0.012548776343464851, 0.004813038278371096, 0.01668519899249077, -0.013127735815942287, -0.009870216250419617, -0.01812213473021984, 0.021051811054348946, 0.02315838634967804, -0.02912934310734272, 0.020675137639045715, -0.0065010893158614635, -0.02347925491631031, -0.019210299476981163, 0.031500983983278275, -0.032282233238220215, 0.04525651037693024, 0.02838994935154915, 0.009165698662400246, -0.050585728138685226, 0.00830772239714861, -0.01190703734755516, -0.009235452860593796, 0.00918662454932928, -0.007833394221961498, 0.01799657754600048, -0.0006487138452939689, -0.006905663292855024, -0.01767570711672306, 0.004502632189542055, -0.01438331138342619, -0.0464283786714077, 0.03563043102622032, 0.007679934613406658, -0.00040413826354779303, 0.012227906845510006, 0.012730137445032597, -0.009709781035780907, -0.004764209967106581, -0.014341458678245544, -0.03348200395703316, 0.013497432693839073, 0.032951872795820236, 0.016196919605135918, 0.010267814621329308, 0.05301317200064659, -0.009109895676374435, -0.003515610471367836, 0.03010590188205242, -0.00865649338811636, -0.010818872600793839, -0.03390052914619446, 0.02279566414654255, 0.003222642932087183, 0.021874910220503807, 0.02993849292397499, -0.014620475471019745, 0.01456467155367136, -0.0030761591624468565, 0.01678285375237465, 0.022335287183523178, -0.0047014313749969006, -0.03543512150645256, 0.003320298856124282, -0.02303282916545868, 0.005189710762351751, 0.015192459337413311, 0.002535564359277487, 0.01498319674283266, -0.026994865387678146, -0.008014754392206669, 0.01840115152299404, 0.003559206845238805, 0.015290115028619766, -0.00289828609675169, 0.033119283616542816, 0.0073799919337034225, 0.0005187966744415462, -0.01794077455997467, 0.027859818190336227, -0.014250777661800385, 0.0037632377352565527, -0.0292967539280653, 0.013678793795406818, 0.0176896583288908, -0.0007742713205516338, -0.002488480182364583, -0.003243569051846862, 0.003982963506132364, -0.0024204698856920004, 0.003285421757027507, -0.031417280435562134, -0.006902175489813089, 0.010811896994709969, -6.484958703367738e-06, -0.007317212875932455, -0.04026211053133011, -0.004070156253874302, 0.010393371805548668, 0.002948857843875885, -0.007756664417684078, -0.005318755749613047, 0.005970957223325968, -0.013671818189322948, -0.03934135660529137, 0.001734263147227466, -0.010023674927651882, 0.019712530076503754, 0.019600924104452133, 0.013476506806910038, 0.01813608594238758, 0.018596462905406952, 0.003442368470132351, -0.026478685438632965, 0.021860959008336067, -0.008768100291490555, -0.0050362516194581985, 0.027273882180452347, -0.012974276207387447, -0.025306815281510353, 0.03010590188205242, -0.005238538607954979, -0.0292967539280653, -0.02965947613120079, 0.022405041381716728, -0.005433850456029177, 0.016545690596103668, -0.01199074275791645, -0.012590629048645496, -0.03708131983876228, 0.011893087066709995, 0.0030970852822065353, 0.0174106415361166, -0.002648914698511362, 0.004066668450832367, -0.01834534853696823, 0.02868291735649109, -0.02357691153883934, 0.010212011635303497, -0.004903718363493681, 0.0064243595115840435, 0.025850897654891014, -0.004160836338996887, 0.022126024588942528, 0.0038155533839017153, 0.030468624085187912, 0.04302437603473663, 0.00297675933688879, 0.008614640682935715, 0.0016810756642371416, -0.002648914698511362, -0.0175501499325037, -0.00871229637414217, -0.021637745201587677, 0.032393839210271835, -0.021735401824116707, -0.001742110587656498, -0.003149400930851698, -0.012151177041232586, -0.0018153524724766612, 0.002219926565885544, -0.0009495287085883319, 0.002467554062604904, -0.003372614271938801, -0.0040841070003807545, 0.0348212830722332, 0.009633051231503487, -0.007965927012264729, -0.0036307049449533224, -0.0056117232888937, -0.002270498313009739, -0.021065760403871536, 0.0037911394611001015, 0.008028705604374409, -0.04595405235886574, -0.02287936955690384, 0.01506690215319395, 0.006644085049629211, 0.020954154431819916, -0.02329789474606514, 0.005336194299161434, 0.002750058425590396, 0.026478685438632965, -0.0058349366299808025, -0.01667124778032303, 0.013392801396548748, -0.0031511448323726654, -0.02991059049963951, 0.004858378320932388, -0.012276735156774521, -0.000695361930411309, 0.015834197402000427, 7.373234257102013e-05, -0.005416411906480789, -0.014759983867406845, 0.015973705798387527, -0.019098693504929543, 0.024553470313549042, -0.004178274888545275, -0.005943055730313063, -0.013902007602155209, 0.007875245995819569, -0.02960367128252983, -0.005677989684045315, -0.004645627923309803, 0.004830476827919483, 0.009263354353606701, 0.024832487106323242, -0.027455244213342667, -0.005597772542387247, -0.006497601512819529, 0.012304636649787426, 0.002464066259562969, -0.015485427342355251, 0.01456467155367136, -0.007826418615877628, 0.004314295947551727, -0.00182058394420892, -0.009451691061258316, 0.0029662963934242725, 0.021330825984477997, -0.014132196083664894, 0.00890760775655508, 0.012402292340993881, -0.015806296840310097, -0.00900526437908411, 0.018749922513961792, 0.034542266279459, 0.0034510877449065447, -0.03130567446351051, -0.019042890518903732, -0.021944664418697357, -0.009772559627890587, -0.021386630833148956, -0.02325604297220707, -0.0035522314719855785, 0.022418992593884468, 0.021414531394839287, -0.03604895621538162, -0.0299942959100008, -0.02822253853082657, -0.020479826256632805, 0.007910123094916344, 0.0038957707583904266, 0.0005327475373633206, 0.010205036029219627, 0.021051811054348946, -0.026841405779123306, 0.05965377017855644, 0.0031424255575984716, -0.005597772542387247, -0.0017708841478452086, 0.020591432228684425, -0.024679027497768402, 0.002418725984171033, -0.007296286523342133, -0.010909552685916424, -0.06579213589429855, 0.0168526079505682, 0.025516077876091003, -0.008203091099858284, 0.031249871477484703, 0.013922933489084244, -0.004649115726351738, 0.01177450455725193, 0.0008989569614641368, 0.02367456629872322, 0.03465387597680092, 0.00436661159619689, 0.007519500330090523, -0.02375827170908451, -0.012541800737380981, 0.004844427574425936, -0.004586337134242058, -0.009974846616387367, 0.004268955439329147, 0.013867130503058434, -0.0006020657019689679, -0.027022767812013626, -0.010267814621329308, 0.009451691061258316, 0.0067208148539066315, -0.027469193562865257, 0.006490625906735659, -0.006218584720045328, -0.028264392167329788, -0.005761695094406605, 0.02253059856593609, -0.0074985739775002, -0.026757702231407166, 0.006891712546348572, -0.0055070919916033745, -0.01668519899249077, 0.037444040179252625, 0.007062610238790512, -0.00045165829942561686, -0.009723732247948647, -0.0029593207873404026, 0.003871356835588813, -0.001362822251394391, 0.004746771417558193, 0.01817793771624565, -0.010037626139819622, 0.022195778787136078, 0.006745228543877602, -0.03596525266766548, 0.008244943805038929, -0.010839798487722874, 0.009444715455174446, -0.0022478282917290926, -0.020758843049407005, 0.008468156680464745, -0.03624426946043968, 0.030329115688800812, 0.0028773597441613674, -0.043917227536439896, -0.02299097552895546, -0.003969012759625912, 0.028264392167329788, -0.012995203025639057, 0.018959185108542442, 0.22287854552268982, -0.028278343379497528, 0.01190703734755516, 0.006745228543877602, 0.014746032655239105, 0.0021309901494532824, 0.03415164351463318, 0.005468727555125952, -0.021233171224594116, -0.005957006476819515, 0.011244373396039009, -0.0011160668218508363, -0.019977595657110214, 0.006183707620948553, 0.00561869889497757, -0.03133357688784599, -0.037974171340465546, -0.02912934310734272, -0.01165592297911644, -0.007903148420155048, 0.018945233896374702, 0.00714980298653245, -0.0014378079213202, -0.024497665464878082, 0.01197679154574871, -0.007519500330090523, -0.009863240644335747, -0.020521678030490875, 0.0018275594338774681, 0.03504449874162674, -0.020033398643136024, 0.009842313826084137, 0.025976454839110374, 0.015555181540548801, -0.005684965290129185, -0.022112073376774788, 0.006120929028838873, -0.005684965290129185, 0.020437972620129585, -0.004683992825448513, 0.025027798488736153, -0.011418758891522884, 0.003665581811219454, -0.008014754392206669, -0.011083938181400299, 0.012876620516180992, -0.008949460461735725, 0.0073381392285227776, -0.005684965290129185, 0.013657867908477783, -0.013476506806910038, 0.016252722591161728, 0.019768333062529564, 0.026520537212491035, -0.0016540458891540766, -0.0030639520846307278, 0.003975987900048494, 0.020382169634103775, 0.022349238395690918, 0.00830074679106474, -0.027929572388529778, 0.021651696413755417, 0.007519500330090523, 0.031752102077007294, 0.02830624394118786, 0.004457291681319475, 0.004799087066203356, 0.00885180477052927, 0.01870807074010372, -0.00428988179191947, -0.032617051154375076, -0.01739669218659401, 0.009605149738490582, -0.0069719296880066395, -0.038448501378297806, -0.01686655916273594, 0.02963157370686531, 0.04291277006268501, 0.03487708792090416, 0.03493289276957512, -0.01163499616086483, -0.01428565476089716, 0.03404003754258156, -0.013999663293361664, -0.03005009889602661, -0.025320766493678093, -0.008070558309555054, -0.013323047198355198, -0.009151747450232506, -0.0027169252280145884, 0.005765182431787252, -0.006316240411251783, -0.025850897654891014, 0.002695998875424266, -0.013650892302393913, -0.005402460694313049, -0.0028076055459678173, 0.032365936785936356, 0.015903951600193977, -0.012597603723406792, -0.03727663308382034, 0.030273312702775, 0.043275490403175354, 0.020661186426877975, 0.0026698410511016846, 0.01707582175731659, -0.0238419771194458, -0.0032801900524646044, -0.008956436067819595, 0.012618530541658401, 0.006340654566884041, -0.03323088958859444, 0.00858673918992281, 0.006630134303122759, -0.02808303013443947, -0.015429623425006866, -0.002199000446125865, -0.01842905394732952, 0.00906106736510992, -0.0020228710491210222, 0.01445306558161974, -0.03498869389295578, -0.013106809929013252, 0.011118815280497074, 0.007972902618348598, 0.0018188401591032743, -0.018819676712155342, -0.020131055265665054, -0.01735483855009079, -0.005311780609190464, 0.012709210626780987, -0.03465387597680092, 0.0369139090180397, -0.009953920729458332, -0.0017098492244258523, 0.00452355807647109, -0.0041154962964355946, -0.005332706496119499, -0.0026698410511016846, 0.005004861857742071, 0.030803443863987923, -0.006406920962035656, -0.0028337633702903986, -0.01746644638478756, 0.020033398643136024, -0.019935743883252144, 0.004035279154777527, -0.0012843487784266472, 0.021093662828207016, -0.02386987954378128, 0.004471242427825928, 0.002577416831627488, 0.0018188401591032743, -0.0178291667252779, 0.01774546131491661, -0.008391426876187325, -0.02385592833161354, -0.027762161567807198, -0.002425701357424259, 0.000267681636614725, -0.02332579717040062, 0.0008954692166298628, 0.00589771568775177, -0.009402862749993801, -0.02260035276412964, -0.02416284568607807, -0.1784590780735016, 0.002692511072382331, 0.026883259415626526, -0.0297152791172266, 0.00711841369047761, 0.027985375374555588, 0.032338034361600876, 0.013009153306484222, -0.016141116619110107, -0.027455244213342667, -0.0018449979834258556, -0.009939970448613167, -0.032784461975097656, -0.0183872003108263, -0.01808028295636177, 0.020996006205677986, -0.010944429785013199, 0.003198229009285569, 0.00864951778203249, 0.019182398915290833, 0.019600924104452133, -0.038783323019742966, -0.0017822191584855318, -0.005388509947806597, 0.01840115152299404, -0.005092054605484009, -0.005489653442054987, 0.00863556656986475, -0.026018306612968445, -0.01674100197851658, -0.006727789994329214, 0.025655586272478104, -0.0030220996122807264, -0.020968105643987656, 0.00231932639144361, 0.022153927013278008, 0.02890613116323948, -0.004603775683790445, -0.01157221756875515, 0.009939970448613167, 0.010463126003742218, -0.0015485426411032677, -0.007965927012264729, -0.021400580182671547, -0.027008816599845886, 0.0060930270701646805, 0.022251581773161888, 0.0017961700214073062, 0.0038364797364920378, 0.013350949622690678, -0.020898351445794106, -0.003484221175312996, 0.024748781695961952, 0.007114925887435675, 0.0167130995541811, -0.003238337580114603, 0.021888859570026398, 0.01729903556406498, -0.014759983867406845, 0.011328077875077724, 0.0008793385932222009, -0.02979898452758789, 0.012534825131297112, -0.0025442836340516806, -0.02363271452486515, -0.01764780655503273, -0.009019214659929276, -0.01833139732480049, -0.022279484197497368, 0.014815786853432655, -0.02392568252980709, -0.007707836572080851, -0.01156524196267128, -0.015192459337413311, 0.021637745201587677, 0.0020089203026145697, -0.016322476789355278, 0.00714980298653245, 0.004108521156013012, 0.005078103858977556, -0.031752102077007294, 0.03010590188205242, -0.007882221601903439, 0.018526708707213402, 0.0058454000391066074, -0.00711143808439374, -0.001628760015591979, -0.010511954315006733, -0.03512820228934288, -0.006881249602884054, 0.02229343354701996, -0.004813038278371096, 0.011125790886580944, 0.0007258793921209872, -0.0008836982306092978, 2.38281354540959e-05, 0.008747173473238945, -0.01220698095858097, 0.024302354082465172, -0.0008645158377476037, -0.006577818654477596, 0.00872624758630991, -0.005430362652987242, 0.015415673144161701, 0.025641635060310364, 0.004774673376232386, -0.00822401698678732, 0.005936080124229193, 0.02893403172492981, 5.983600203762762e-05, 0.003808578010648489, -0.016294576227664948, -0.0002992890076711774, 0.012932424433529377, 0.009514469653367996, 0.01220698095858097, 0.0016540458891540766, -0.008879706263542175, 0.007861295714974403, -0.012548776343464851, 0.030942952260375023, -0.01130715198814869, 0.005615211091935635, 0.002343740314245224, -0.009040141478180885, -0.021819105371832848, -0.11009999364614487, 0.0035034033935517073, 0.008942484855651855, 0.00879600178450346, -0.02322814054787159, -0.011941914446651936, 0.00850303377956152, 0.0232839435338974, -0.025418421253561974, 0.015555181540548801, -0.02257245033979416, -0.024888290092349052, 0.037053417414426804, -0.014208925887942314, 0.0020089203026145697, 0.0009068042854778469, 0.008558837696909904, -0.006365068722516298, -0.021456385031342506, 0.030357016250491142, 0.013336998410522938, -0.009437739849090576, 0.010253863409161568, -0.006508064456284046, 0.016113214194774628, 0.01129320077598095, -0.0451728031039238, 0.0028145809192210436, 0.01722928136587143, 0.009472616948187351, 0.008217042312026024, -0.0457308366894722, 0.010428248904645443, -0.01750829815864563, -0.014732081443071365, -0.021009957417845726, -0.013734596781432629, -0.008621616289019585, 0.016503838822245598, -0.031612593680620193, -0.006860323250293732, 0.00421315198764205, 0.010142257437109947, -0.01690841279923916, -0.007233507931232452, -0.012751063331961632, -0.007100975140929222, 0.05223192647099495, 0.01470417995005846, -0.02816673554480076, -0.05248304083943367, -0.012269759550690651, -0.025557929649949074, 0.010909552685916424, 0.02921304851770401, 0.0225864015519619, 0.007665983866900206, -0.005486166104674339, -0.008544886484742165, 0.0010707265464589, -0.01718742959201336, -0.014815786853432655, -0.011886111460626125, 0.02359086275100708, 0.031752102077007294, -0.0021676109172403812, -0.038085781037807465, -0.015401721931993961, -0.014278680086135864, -0.011349004693329334, 0.0030761591624468565, 0.003993426449596882, 0.0027134374249726534, 0.010023674927651882, -0.031221969053149223, -0.027776112779974937, -0.01207444816827774, -0.020647235214710236, 0.0022565475665032864, -0.007254434283822775, -0.0031546326354146004, -0.019391661509871483, -0.002394312061369419, -0.020117104053497314, -0.010337568819522858, -0.013413728214800358, -0.005772158037871122, -0.015820246189832687, -0.020047349855303764, -0.015876051038503647, -0.003927160054445267, 0.019893890246748924, -0.012220931239426136, -0.008049631491303444, -0.01731298677623272, -0.009312182664871216, -0.015094803646206856, 0.00425849249586463, 0.027287833392620087, 0.01213025115430355, -0.0073451148346066475, -0.004659579135477543, -0.043777719140052795, 0.011335053481161594, 0.014090343378484249, -0.026143865659832954, 0.007979877293109894, 0.0020577481482177973, 0.02830624394118786, 0.00032174112857319415, -0.010044600814580917, -0.0021222708746790886, -0.011132766492664814, 0.00742881977930665, -0.024595322087407112, -0.0291572455316782, -0.013818302191793919, -0.0070172701962292194, 0.027050668373703957, -0.00913779716938734, 0.009374961256980896, -0.00011138869740534574, -0.015778394415974617, -0.007979877293109894, 0.009681879542768002, 0.030245410278439522, -0.005925617180764675, -0.0013262012507766485, 0.007254434283822775, 0.026436831802129745, -0.010693315416574478, -0.003163351910188794, 0.013692745007574558, -0.025920651853084564, -0.00871229637414217, 0.02823648974299431, -0.011676848866045475, -0.00592910498380661, 0.016155067831277847, 0.0076380823738873005, 0.03339830040931702, 0.01848485693335533, -0.020033398643136024, -0.02368851751089096, 0.01725718379020691, 0.0063894824124872684, -0.01753620058298111, 0.013371875509619713, 0.01157221756875515, 0.003993426449596882, 0.01686655916273594, -0.01121647097170353, 0.013525334186851978, 0.024776682257652283, 0.0052943420596420765, -0.015792345628142357, -0.0355188250541687, 0.007477647624909878, -0.002472785534337163, -0.006923101842403412, 0.005831449292600155, 0.0018101208843290806, 0.032338034361600876, 0.007024245336651802, 0.019001036882400513, 0.014829738065600395, 0.0006007578340359032, -0.015736542642116547, 0.006330191623419523, 0.0029976856894791126, -0.007435794919729233, -0.04095965251326561, -0.01668519899249077, 0.0290177371352911, -0.008098459802567959, 0.009842313826084137, 0.020926252007484436, -0.0015851636417210102, -0.013030080124735832, 0.001406418508850038, 0.007317212875932455, 0.01845695450901985, 0.01159314438700676, -0.01854065991938114, -0.016210870817303658, -0.0029279314912855625, 0.02331184595823288, 0.019754381850361824, 0.00878205057233572, 0.004443340934813023, -0.013602063991129398, 0.025167306885123253, 0.015708640217781067, 0.008161238394677639, -0.004199201241135597, 0.019056839868426323, -0.00422710319980979, 0.03507240116596222, -0.013727622106671333, -0.010135281831026077, 0.013539285399019718, -0.005775645840913057, 0.013504408299922943, 0.0046630664728581905, -0.005057177506387234, -0.00749159837141633, -0.025906700640916824, 0.01151641458272934, -0.015499377623200417, -0.016099262982606888, -0.01143968477845192, 0.019252153113484383, 0.00011454943160060793, 0.004861866123974323, -0.008670443668961525, 0.00020392196893226355, -0.015234312042593956, 0.02260035276412964, 0.006930077448487282, -0.061104658991098404, -0.02836204692721367, 0.007366040721535683, 0.030691837891936302, 0.012165128253400326, 0.042801160365343094, 0.004003889858722687, 0.02876662276685238, 0.03392843157052994, 0.006989368237555027, -0.019168447703123093, 0.03345410153269768, 0.009019214659929276, -0.0004425030492711812, -0.0004241925780661404, 0.01710372418165207, -0.007617156021296978, -0.005963982082903385, -0.013030080124735832, -0.02424655109643936, 0.01172567717730999, -0.01855461113154888, 0.0784594938158989, 0.020870449021458626, -0.013971760869026184, -0.002593111479654908, -0.032365936785936356, -0.010623561218380928, 0.018680168315768242, 0.0224468931555748, -0.015359869226813316, -0.014341458678245544, -0.0028267879970371723, -0.0008274589199572802, -0.02296307496726513, -0.02282356657087803, 0.014097318984568119, -0.026046209037303925, -0.004101545549929142, 0.003878332208842039, -0.007819443009793758, -0.01185123436152935, 0.01653173938393593, 0.00599188357591629, 0.0035609507467597723, 0.008482107892632484, -0.004195713438093662, -0.021330825984477997, 0.031277772039175034, -0.00429685739800334, -0.005364096257835627, -0.026269422844052315, -0.004014352802187204, 0.0076729594729840755, -0.038029976189136505, -0.019503267481923103, 0.010281765833497047, -0.0013497433392331004, 0.0003999966138508171, 0.00844723079353571, 0.014278680086135864, 0.015848148614168167, -0.0007716555846855044, 0.01689446158707142, -0.019865989685058594, -0.0295757707208395, -0.038727518171072006, -0.002964552491903305, -0.0012485997285693884, -0.025153355672955513, -0.032365936785936356]\n"
     ]
    }
   ],
   "source": [
    "question = 'Which training method should I use for sentence transformers when I only have pairs of related sentences?'\n",
    "question_embedding = openai.Embedding.create(\n",
    "    input=[question], engine=embed_model\n",
    ")\n",
    "question_embedding_vector = question_embedding['data'][0]['embedding']\n",
    "print(question_embedding_vector)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us see what we get when we search our vector databases."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pinecone\n",
    "Using the `query` function, we can find the 2 closest matches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'matches': [{'id': 'pNvujJ1XyeQ-t418.88',\n",
      "              'metadata': {'channel_id': 'UCv83tO5cePwHMt1952IVVHw',\n",
      "                           'end': 568.4,\n",
      "                           'published': '2021-11-24 16:24:24 UTC',\n",
      "                           'start': 418.88,\n",
      "                           'text': 'pairs of related sentences you can go '\n",
      "                                   'ahead and actually try training or '\n",
      "                                   'fine-tuning using NLI with multiple '\n",
      "                                   \"negative ranking loss. If you don't have \"\n",
      "                                   'that fine. Another option is that you have '\n",
      "                                   'a semantic textual similarity data set or '\n",
      "                                   'STS and what this is is you have so you '\n",
      "                                   'have sentence A here, sentence B here and '\n",
      "                                   'then you have a score from from 0 to 1 '\n",
      "                                   'that tells you the similarity between '\n",
      "                                   'those two scores and you would train this '\n",
      "                                   'using something like cosine similarity '\n",
      "                                   \"loss. Now if that's not an option and your \"\n",
      "                                   'focus or use case is on building a '\n",
      "                                   'sentence transformer for another language '\n",
      "                                   'where there is no current sentence '\n",
      "                                   'transformer you can use multilingual '\n",
      "                                   'parallel data. So what I mean by that is '\n",
      "                                   'so parallel data just means translation '\n",
      "                                   'pairs so if you have for example a English '\n",
      "                                   'sentence and then you have another '\n",
      "                                   'language here so it can it can be anything '\n",
      "                                   \"I'm just going to put XX and that XX is \"\n",
      "                                   'your target language you can fine-tune a '\n",
      "                                   'model using something called multilingual '\n",
      "                                   'knowledge distillation and what that does '\n",
      "                                   'is takes a monolingual model for example '\n",
      "                                   'in English and using those translation '\n",
      "                                   'pairs it distills the knowledge the '\n",
      "                                   'semantic similarity knowledge from that '\n",
      "                                   'monolingual English model into a '\n",
      "                                   'multilingual model which can handle both '\n",
      "                                   'English and your target language. So '\n",
      "                                   \"they're three options quite popular very \"\n",
      "                                   'common that you can go for and as a '\n",
      "                                   'supervised methods the chances are that '\n",
      "                                   'probably going to outperform anything you '\n",
      "                                   'do with unsupervised training at least for '\n",
      "                                   'now. So if none of those sound like '\n",
      "                                   'something',\n",
      "                           'title': 'Today Unsupervised Sentence Transformers, '\n",
      "                                    'Tomorrow Skynet (how TSDAE works)',\n",
      "                           'url': 'https://youtu.be/pNvujJ1XyeQ'},\n",
      "              'score': 0.8653607,\n",
      "              'sparse_values': {'indices': [], 'values': []},\n",
      "              'values': []},\n",
      "             {'id': 'WS1uVMGhlWQ-t737.28',\n",
      "              'metadata': {'channel_id': 'UCv83tO5cePwHMt1952IVVHw',\n",
      "                           'end': 900.72,\n",
      "                           'published': '2021-10-20 17:06:20 UTC',\n",
      "                           'start': 737.28,\n",
      "                           'text': \"were actually more accurate. So we can't \"\n",
      "                                   \"really do that. We can't use this what is \"\n",
      "                                   'called a mean pooling approach. Or we '\n",
      "                                   \"can't use it in its current form. Now the \"\n",
      "                                   'solution to this problem was introduced by '\n",
      "                                   'two people in 2019 Nils Reimers and Irenia '\n",
      "                                   'Gurevich. They introduced what is the '\n",
      "                                   'first sentence transformer or sentence '\n",
      "                                   'BERT. And it was found that sentence BERT '\n",
      "                                   'or S BERT outformed all of the previous '\n",
      "                                   'Save the Art models on pretty much all '\n",
      "                                   'benchmarks. Not all of them but most of '\n",
      "                                   'them. And it did it in a very quick time. '\n",
      "                                   'So if we compare it to BERT, if we wanted '\n",
      "                                   'to find the most similar sentence pair '\n",
      "                                   'from 10,000 sentences in that 2019 paper '\n",
      "                                   'they found that with BERT that took 65 '\n",
      "                                   'hours. With S BERT embeddings they could '\n",
      "                                   'create all the embeddings in just around '\n",
      "                                   'five seconds. And then they could compare '\n",
      "                                   'all those with cosine similarity in 0.01 '\n",
      "                                   \"seconds. So it's a lot faster. We go from \"\n",
      "                                   '65 hours to just over five seconds which '\n",
      "                                   'is I think pretty incredible. Now I think '\n",
      "                                   \"that's pretty much all the context we need \"\n",
      "                                   'behind sentence transformers. And what we '\n",
      "                                   'do now is dive into a little bit of how '\n",
      "                                   'they actually work. Now we said before we '\n",
      "                                   'have the core transform models and what S '\n",
      "                                   'BERT does is fine tunes on sentence pairs '\n",
      "                                   'using what is called a Siamese '\n",
      "                                   'architecture or Siamese network. What we '\n",
      "                                   'mean by a Siamese network is that we have '\n",
      "                                   'what we can see, what can view as two BERT '\n",
      "                                   'models that are identical and the weights '\n",
      "                                   'between those two models are tied. Now in '\n",
      "                                   'reality when implementing this we just use '\n",
      "                                   'a single BERT model. And what we do is we '\n",
      "                                   'process one sentence, a sentence A through '\n",
      "                                   'the model and then we process another '\n",
      "                                   'sentence, sentence B through the model. '\n",
      "                                   \"And that's the sentence pair. So with our \"\n",
      "                                   'cross-linked we were processing the '\n",
      "                                   'sentence pair together. We were putting '\n",
      "                                   'them both together, processing them all at '\n",
      "                                   'once. This time we process them '\n",
      "                                   'separately. And during training what '\n",
      "                                   'happens is the weights',\n",
      "                           'title': 'Intro to Sentence Embeddings with '\n",
      "                                    'Transformers',\n",
      "                           'url': 'https://youtu.be/WS1uVMGhlWQ'},\n",
      "              'score': 0.85877204,\n",
      "              'sparse_values': {'indices': [], 'values': []},\n",
      "              'values': []}],\n",
      " 'namespace': ''}\n"
     ]
    }
   ],
   "source": [
    "pinecone_index_name = \"rag-qa\"\n",
    "pinecone_index = pinecone.GRPCIndex(pinecone_index_name)\n",
    "\n",
    "pinecone_results = pinecone_index.query(question_embedding_vector, top_k=2, include_metadata=True)\n",
    "print(pinecone_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a `dict` with a `matches` list containing two elements. The `score` is a Pinecone-derived value documented on the [query API](https://docs.pinecone.io/reference/query):\n",
    "\n",
    "If you look at the `metadata.text`, you will see this is the transcript text that we combined above:\n",
    "\n",
    "```\n",
    "{'matches': [{'id': 'pNvujJ1XyeQ-t418.88',\n",
    "              'metadata': {'channel_id': 'UCv83tO5cePwHMt1952IVVHw',\n",
    "                           'end': 568.4,\n",
    "                           'published': '2021-11-24 16:24:24 UTC',\n",
    "                           'start': 418.88,\n",
    "                           'text': 'pairs of related sentences you can go '\n",
    "                                   'ahead and actually try training or '\n",
    "                                   'fine-tuning using NLI with ...',\n",
    "                           'title': 'Today Unsupervised Sentence Transformers, '\n",
    "                                    'Tomorrow Skynet (how TSDAE works)',\n",
    "                           'url': 'https://youtu.be/pNvujJ1XyeQ'},\n",
    "              'score': 0.865344,\n",
    "              'sparse_values': {'indices': [], 'values': []},\n",
    "              'values': []},\n",
    "             {'id': 'WS1uVMGhlWQ-t737.28',\n",
    "              'metadata': {'channel_id': 'UCv83tO5cePwHMt1952IVVHw',\n",
    "                           'end': 900.72,\n",
    "                           'published': '2021-10-20 17:06:20 UTC',\n",
    "                           'start': 737.28,\n",
    "                           'text': \"were actually more accurate. So we can't \"\n",
    "                                   \"really do that. We can't use this what is \"\n",
    "                                   'called a mean pooling approach ...',\n",
    "                           'title': 'Intro to Sentence Embeddings with '\n",
    "                                    'Transformers',\n",
    "                           'url': 'https://youtu.be/WS1uVMGhlWQ'},\n",
    "              'score': 0.8586723,\n",
    "              'sparse_values': {'indices': [], 'values': []},\n",
    "              'values': []}],\n",
    " 'namespace': ''}\n",
    " ```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's finally define a helper function that will return a string of combined `text` for a given embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pinecone_context(vector_embedding: list[float], top_k: int):\n",
    "    pinecone_results = pinecone_index.query(vector_embedding, top_k=top_k, include_metadata=True)\n",
    "    return \" \".join(result['metadata']['text'] for result in pinecone_results['matches'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Astra\n",
    "As Astra is a Cassandra database with a Vector-enabled SAI, we can search using the `ANN OF` filter and provide `question_embedding_vector` as a parameter. In this query, we are also computing a scalar `distance` using a similarity cosine; the closer to `1` the more similar the two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pNvujJ1XyeQ-t418.88 0.9326715469360352 pairs of related sentences you can go ahead and actually try training or fine-tuning using NLI with multiple negative ranking loss. If you don't have that fine. Another option is that you have a semantic textual similarity data set or STS and what this is is you have so you have sentence A here, sentence B here and then you have a score from from 0 to 1 that tells you the similarity between those two scores and you would train this using something like cosine similarity loss. Now if that's not an option and your focus or use case is on building a sentence transformer for another language where there is no current sentence transformer you can use multilingual parallel data. So what I mean by that is so parallel data just means translation pairs so if you have for example a English sentence and then you have another language here so it can it can be anything I'm just going to put XX and that XX is your target language you can fine-tune a model using something called multilingual knowledge distillation and what that does is takes a monolingual model for example in English and using those translation pairs it distills the knowledge the semantic similarity knowledge from that monolingual English model into a multilingual model which can handle both English and your target language. So they're three options quite popular very common that you can go for and as a supervised methods the chances are that probably going to outperform anything you do with unsupervised training at least for now. So if none of those sound like something\n",
      "WS1uVMGhlWQ-t737.28 0.9293355941772461 were actually more accurate. So we can't really do that. We can't use this what is called a mean pooling approach. Or we can't use it in its current form. Now the solution to this problem was introduced by two people in 2019 Nils Reimers and Irenia Gurevich. They introduced what is the first sentence transformer or sentence BERT. And it was found that sentence BERT or S BERT outformed all of the previous Save the Art models on pretty much all benchmarks. Not all of them but most of them. And it did it in a very quick time. So if we compare it to BERT, if we wanted to find the most similar sentence pair from 10,000 sentences in that 2019 paper they found that with BERT that took 65 hours. With S BERT embeddings they could create all the embeddings in just around five seconds. And then they could compare all those with cosine similarity in 0.01 seconds. So it's a lot faster. We go from 65 hours to just over five seconds which is I think pretty incredible. Now I think that's pretty much all the context we need behind sentence transformers. And what we do now is dive into a little bit of how they actually work. Now we said before we have the core transform models and what S BERT does is fine tunes on sentence pairs using what is called a Siamese architecture or Siamese network. What we mean by a Siamese network is that we have what we can see, what can view as two BERT models that are identical and the weights between those two models are tied. Now in reality when implementing this we just use a single BERT model. And what we do is we process one sentence, a sentence A through the model and then we process another sentence, sentence B through the model. And that's the sentence pair. So with our cross-linked we were processing the sentence pair together. We were putting them both together, processing them all at once. This time we process them separately. And during training what happens is the weights\n"
     ]
    }
   ],
   "source": [
    "KEYSPACE_NAME = 'vsearch'\n",
    "TABLE_NAME = 'youtube_transcriptions'\n",
    "\n",
    "from cassandra.query import SimpleStatement\n",
    "\n",
    "cql_query = SimpleStatement(\n",
    "    f\"SELECT id, similarity_cosine(embedding, %s) AS distance, transcript_text FROM {KEYSPACE_NAME}.{TABLE_NAME} ORDER BY embedding ANN OF %s LIMIT %s\"\n",
    ")\n",
    "res = session.execute(cql_query, (question_embedding_vector, question_embedding_vector, 2))\n",
    "rows = [row for row in res]\n",
    "for row in rows:\n",
    "    print (row.id, row.distance, row.transcript_text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have results similar to:\n",
    "\n",
    "```\n",
    "pNvujJ1XyeQ-t418.88 0.9326715469360352 pairs of related sentences you can go ahead and actually try training or fine-tuning using NLI with ...\n",
    "WS1uVMGhlWQ-t737.28 0.9293355941772461 were actually more accurate. So we can't really do that. We can't use this what is called a mean pooling ...\n",
    "```\n",
    "\n",
    "And if you compare the IDs with those in the Pinecone results you may note they are the same results. While this is comforting, know that there is no guarantee that this will always be the same!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finally define a helper function that will return a string of combined `transcript_text` for a given embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def astra_context(vector_embedding: list[float], top_k: int):\n",
    "    cql_query = SimpleStatement(\n",
    "        f\"SELECT id, similarity_cosine(embedding, %s) AS distance, transcript_text FROM {KEYSPACE_NAME}.{TABLE_NAME} ORDER BY embedding ANN OF %s LIMIT %s\"\n",
    "    )\n",
    "    res = session.execute(cql_query, (vector_embedding, vector_embedding, top_k))\n",
    "    return \" \".join(row.transcript_text for row in res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Completion, With Context\n",
    "We now have all the pieces to complete Retrieval-Augmented Generation. We're basically going to ask the Completion:\n",
    "\n",
    "> Answer the question based on the context below.\n",
    ">\n",
    "> Context: {Text retrieved from vector database}\n",
    ">\n",
    "> Question: {Original question}\n",
    ">\n",
    "> Answer: \n",
    "\n",
    "We will need to be considerate of the total number of tokens (words and punctuation) the Completion model supports. For example, `text-davinci-003` suppoorts a maximum of 4097 tokens. OpenAI provides an open-source token counter we can use to make sure we do not go over the limit; they document its usage in a [cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token list:  [31373, 11, 995, 0]\n",
      "decoded token list:  hello, world!\n",
      "decoded token list that has been truncated:  hello,\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "token_encoder = tiktoken.encoding_for_model(\"text-davinci-003\")\n",
    "token_encoding = token_encoder.encode(\"hello, world!\")\n",
    "print(\"token list: \",token_encoding)\n",
    "print(\"decoded token list: \",token_encoder.decode(token_encoding))\n",
    "print(\"decoded token list that has been truncated: \", token_encoder.decode(token_encoding[:2]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the given model `text-davinci-003`, we expect to see four tokens:\n",
    "```\n",
    "[31373, 11, 995, 0]\n",
    "```\n",
    "\n",
    "We can also see the decoder returns back our original string, and that we can truncate the token list (`[:2]`) to eliminate tokens, and still result in meaningful text.\n",
    "\n",
    "We will now create a template question, based on the above, and compute the number of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "def with_context(question: str, context: str):\n",
    "    return f\"\"\"Answer the question based on the context below.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer: \"\"\"\n",
    "\n",
    "question_with_context = with_context(question, \"\")\n",
    "question_with_context_tokens = token_encoder.encode(question_with_context)\n",
    "print(len(question_with_context_tokens))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding this to be `39`, we can add up to `4097 - 39 = 4058` tokens of context."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pinecone\n",
    "Putting it all together, we:\n",
    "1. Get the added context from Pinecone with a vector search, using the vector embedding of our question;\n",
    "2. We trim this context to `4058` tokens;\n",
    "3. We call the Completion API with our original question and additional context within our prompt template;\n",
    "4. Print the original question and answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Which training method should I use for sentence transformers when I only have pairs of related sentences?\n",
      "Answer: NLI with multiple negative ranking loss.\n"
     ]
    }
   ],
   "source": [
    "context = pinecone_context(question_embedding_vector, 3)\n",
    "context = token_encoder.decode(token_encoder.encode(context)[:4058])\n",
    "answer = complete(with_context(question,context))\n",
    "\n",
    "print(f\"\"\"Question: {question}\n",
    "Answer: {answer}\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected answer given in the Pinecone example is:\n",
    "\n",
    "> You should use Natural Language Inference (NLI) with multiple negative ranking loss.\n",
    "\n",
    "And the answer given should be similar to this."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Astra\n",
    "Putting it all together, we:\n",
    "1. Get the added context from Astra with a vector search, using the vector embedding of our question;\n",
    "2. We trim this context to `4058` tokens;\n",
    "3. We call the Completion API with our original question and additional context within our prompt template;\n",
    "4. Print the original question and answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Which training method should I use for sentence transformers when I only have pairs of related sentences?\n",
      "Answer: NLI with multiple negative ranking loss.\n"
     ]
    }
   ],
   "source": [
    "context = astra_context(question_embedding_vector, 3)\n",
    "context = token_encoder.decode(token_encoder.encode(context)[:4058])\n",
    "answer = complete(with_context(question,context))\n",
    "\n",
    "print(f\"\"\"Question: {question}\n",
    "Answer: {answer}\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected answer given in the Pinecone example is:\n",
    "\n",
    "> You should use Natural Language Inference (NLI) with multiple negative ranking loss.\n",
    "\n",
    "And the answer given should be similar to this."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Together Now\n",
    "As a final activity, let us define a function that combines all of these steps in one, and ask it to define `NLI`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asking question without context\n",
      "NLI stands for Natural Language Inference. It is a task in natural language processing (NLP) that involves determining whether a given premise entails a given hypothesis.\n",
      "\n",
      "Asking question with context\n",
      "Context: So I fine tune on classification and NLI together and then I look at the performance on NLI on the on the test sets of these tasks, right. And my question is, is that more or less than if I were to just fine tune on NLI, which is this diagonal entry right here, I fine tune on NLI and NLI given like the same compute budget, select one of these two numbers, one, it's sort of the data equivalent to both tasks and one is the compute equivalent. You can choose but given that this cell is green right here, I think the authors have taken have chosen the top number to be sort of the more representative one. In this case, you see that training, switch, sorry, co training classification tasks and NLI tasks benefits NLI task evaluation, compared to just fine tuning on NLI tasks. Okay, that's how you read these, these numbers. Or maybe I'm wrong, but that's how I think you read these numbers. So on the other hand, you look for example, right here, you have this CMNS, I don't actually remember what that family, the name is, it gets a performance of 68.24. If it's just trained by itself, if it's fine tuned by itself, if you add the CBQA family, it gains in perfect Wait a minute, it is green. Column, family J column J. This is a column, right? Could this be actually row? No, this is smaller than this. Why is this green? Why is it green? I'm confused. Maybe they have to compare with the average right here. I'm confused by this table. I thought it meant, I thought it meant, if you co train on the two, actually, it gets better. However, the 66 is clearly smaller than the 68. So, so maybe they consider the row, not the column. I don't know. No, they would do consider the column because they're in the text, they say, look, for example, adding an NLI task, you can see in the row, this is whatever happens to another task. If you add the NLI tasks, this is very often beneficial, right? You can see right here, it's very often a good thing to add the NLI task, for example. what we're going to do here is just have a look at a few of the most popular training approaches and what sort of data we need for that. So the first one we're looking at here is Natural Language Inference or NLI and NLI requires that we have pairs of sentences that are labeled as either contradictory, neutral which means they're not necessarily related or as entailing or as inferring each other. So you have pairs that entail each other so they are both very similar pairs that are neutral and also pairs that are contradictory. And this is the traditional NLI data. Now using another version of fine-tuning with NLI called a multiple negatives ranking loss you can get by with only entailment pairs so pairs that are related to each other or positive pairs and it can also use contradictory pairs to improve the performance of training as well but you don't need it. So if you have positive pairs of related sentences you can go ahead and actually try training or fine-tuning using NLI with multiple negative ranking loss. If you don't have that fine. Another option is that you have a semantic textual similarity data set or STS and what this is is you have so you have sentence A here, sentence B here and then you have a score from from 0 to 1 that tells you the similarity between those two scores and you would train this using something like cosine similarity loss. Now if that's not an option and your focus or use case is on building a sentence transformer for another language where there is no if you view this as interpolating the training data, it suddenly makes sense where it's good and where it isn't good. So this was the super glue. And and NLI, it is performing particularly poorly on NLI, which is the ability to understand the relationship between two sentences, right. So where the model classifies whether the second sentence logically follows from the first contradicts the first or is possibly true neutral. Okay, so this is the reasoning part of this model is not given. It is simply recalling the training data and doing language modeling. Now they say, oh, we can test this, we can test this with synthetic and qualitative tasks. So they invent some own tasks since you know now it's pretty easy since you don't have to fine tune the model, you don't have to generate an actual training set for a task. So you can focus on generating a test set and and you know, that's what they do. So they do something like arithmetic. So they say, okay, can we come up with a bunch of arithmetic tasks, for example, two digit addition. So what the model would see would that this is an example and what the model would see is simply this as a context right here for the prompt. And if you give it examples. So if this is like one shot learning, you would input add the following numbers, the following numbers as a string, right, then a new line. And then you would give it one example, like what is 11 plus 12. And with the answer together with the answer, answer is, I don't even know, 23. And then you, the prompt goes here. So what is 48 plus 76? And then you ask, what is the next word right here? What is the next string token that comes here? Now, the the inference here is that if the model manages to do this, it can't simply because these are all strings, the model basically has no clue how to do math. These are numbers to the model. These are just tokens as strings. And the inference\n",
      "Natural Language Inference (NLI) is a type of fine-tuning that requires pairs of sentences that are labeled as either contradictory, neutral, or entailing/inferring each other.\n"
     ]
    }
   ],
   "source": [
    "def get_answer(question: str, db: str = None, context_depth: int = 3):\n",
    "    question_embedding = openai.Embedding.create(input=[question], engine=embed_model)\n",
    "    question_embedding_vector = question_embedding['data'][0]['embedding']\n",
    "    context = None\n",
    "    match db:\n",
    "        case \"pinecone\":\n",
    "            context = pinecone_context(question_embedding_vector, context_depth)\n",
    "        case \"astra\": \n",
    "            context = astra_context(question_embedding_vector, context_depth)\n",
    "    if context is None:\n",
    "        print(\"Asking question without context\")\n",
    "        answer = complete(question)\n",
    "    else:\n",
    "        context = token_encoder.decode(token_encoder.encode(context)[:4058])\n",
    "        print(\"Asking question with context\")\n",
    "        print(f\"Context: {context}\")\n",
    "        answer = complete(with_context(question,context))\n",
    "    return answer\n",
    "\n",
    "print(get_answer('What is NLI?'))\n",
    "print()\n",
    "print(get_answer('What is NLI?', 'astra')) # you may wish to try 'pinecone' as well"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the difference between giving context or not giving context. Apart from the possibility that \"NLI\" could be a more widely used acronym, when providing the context (from our domain-specific YouTube transcripts), the added context has given us additional detail in the answer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versus ChatGPT\n",
    "As one last demonstration as to the differences of APIs, let us compare our `get_answer` with the ubiquitous ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asking question without context\n",
      "Asking question with context\n",
      "Context: So I fine tune on classification and NLI together and then I look at the performance on NLI on the on the test sets of these tasks, right. And my question is, is that more or less than if I were to just fine tune on NLI, which is this diagonal entry right here, I fine tune on NLI and NLI given like the same compute budget, select one of these two numbers, one, it's sort of the data equivalent to both tasks and one is the compute equivalent. You can choose but given that this cell is green right here, I think the authors have taken have chosen the top number to be sort of the more representative one. In this case, you see that training, switch, sorry, co training classification tasks and NLI tasks benefits NLI task evaluation, compared to just fine tuning on NLI tasks. Okay, that's how you read these, these numbers. Or maybe I'm wrong, but that's how I think you read these numbers. So on the other hand, you look for example, right here, you have this CMNS, I don't actually remember what that family, the name is, it gets a performance of 68.24. If it's just trained by itself, if it's fine tuned by itself, if you add the CBQA family, it gains in perfect Wait a minute, it is green. Column, family J column J. This is a column, right? Could this be actually row? No, this is smaller than this. Why is this green? Why is it green? I'm confused. Maybe they have to compare with the average right here. I'm confused by this table. I thought it meant, I thought it meant, if you co train on the two, actually, it gets better. However, the 66 is clearly smaller than the 68. So, so maybe they consider the row, not the column. I don't know. No, they would do consider the column because they're in the text, they say, look, for example, adding an NLI task, you can see in the row, this is whatever happens to another task. If you add the NLI tasks, this is very often beneficial, right? You can see right here, it's very often a good thing to add the NLI task, for example. if you view this as interpolating the training data, it suddenly makes sense where it's good and where it isn't good. So this was the super glue. And and NLI, it is performing particularly poorly on NLI, which is the ability to understand the relationship between two sentences, right. So where the model classifies whether the second sentence logically follows from the first contradicts the first or is possibly true neutral. Okay, so this is the reasoning part of this model is not given. It is simply recalling the training data and doing language modeling. Now they say, oh, we can test this, we can test this with synthetic and qualitative tasks. So they invent some own tasks since you know now it's pretty easy since you don't have to fine tune the model, you don't have to generate an actual training set for a task. So you can focus on generating a test set and and you know, that's what they do. So they do something like arithmetic. So they say, okay, can we come up with a bunch of arithmetic tasks, for example, two digit addition. So what the model would see would that this is an example and what the model would see is simply this as a context right here for the prompt. And if you give it examples. So if this is like one shot learning, you would input add the following numbers, the following numbers as a string, right, then a new line. And then you would give it one example, like what is 11 plus 12. And with the answer together with the answer, answer is, I don't even know, 23. And then you, the prompt goes here. So what is 48 plus 76? And then you ask, what is the next word right here? What is the next string token that comes here? Now, the the inference here is that if the model manages to do this, it can't simply because these are all strings, the model basically has no clue how to do math. These are numbers to the model. These are just tokens as strings. And the inference what we're going to do here is just have a look at a few of the most popular training approaches and what sort of data we need for that. So the first one we're looking at here is Natural Language Inference or NLI and NLI requires that we have pairs of sentences that are labeled as either contradictory, neutral which means they're not necessarily related or as entailing or as inferring each other. So you have pairs that entail each other so they are both very similar pairs that are neutral and also pairs that are contradictory. And this is the traditional NLI data. Now using another version of fine-tuning with NLI called a multiple negatives ranking loss you can get by with only entailment pairs so pairs that are related to each other or positive pairs and it can also use contradictory pairs to improve the performance of training as well but you don't need it. So if you have positive pairs of related sentences you can go ahead and actually try training or fine-tuning using NLI with multiple negative ranking loss. If you don't have that fine. Another option is that you have a semantic textual similarity data set or STS and what this is is you have so you have sentence A here, sentence B here and then you have a score from from 0 to 1 that tells you the similarity between those two scores and you would train this using something like cosine similarity loss. Now if that's not an option and your focus or use case is on building a sentence transformer for another language where there is no\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Natural Language Inference (NLI) is a type of machine learning task that requires pairs of sentences to be labeled as either contradictory, neutral, or entailing/inferring each other.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(get_answer('Can you define NLI, in the context of ML?'))\n",
    "print(get_answer('Can you define NLI, in the context of ML?', 'astra'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer given at the time of the writing of this notebook:\n",
    "\n",
    "> Natural Language Inference (NLI) is a type of machine learning task that requires pairs of sentences to be labeled as either contradictory, neutral, or entailing/inferring each other.\n",
    "\n",
    "Asking ChatGPT running model GPT-3.5 (May 24, 2023):\n",
    "> In the context of machine learning (ML), NLI stands for Natural Language Inference. It is a task that involves determining the relationship between two given sentences: a premise and a hypothesis. The goal of NLI is to determine whether the hypothesis is entailed, contradicted, or has no relation to the premise. It is also known as Recognizing Textual Entailment (RTE).\n",
    ">\n",
    "> NLI is an important problem in natural language processing (NLP) and has various applications, including question answering, information retrieval, dialogue systems, and sentiment analysis. It helps machines understand and reason about the meaning and relationships between sentences, allowing them to comprehend and generate human-like responses.\n",
    ">\n",
    "> To solve NLI tasks, machine learning models are trained on labeled datasets where the relationships between premises and hypotheses are annotated. These models typically use techniques such as recurrent neural networks (RNNs), convolutional neural networks (CNNs), or transformer-based architectures like BERT or GPT. The trained models can then be used to classify new pairs of sentences into the appropriate relationship categories (entailment, contradiction, or neutral).\n",
    "\n",
    "And model GPT-4 (May 24, 2023):\n",
    "> Sure, in the context of Machine Learning (ML), NLI stands for Natural Language Inference.\n",
    "> \n",
    "> Natural Language Inference is a subfield of Natural Language Processing (NLP), and it's concerned with the task of determining whether a \"hypothesis\" is true (entailment), false (contradiction), or undetermined (neutral) given a \"premise\".\n",
    "> \n",
    "> In other words, it's about teaching machines to understand the relationship between a pair of sentences or pieces of text. Here's an example:\n",
    "> \n",
    "> * Premise: \"The dog is sleeping on the couch.\"\n",
    "> * Hypothesis 1: \"There is a sleeping dog on the furniture.\" (Entailment)\n",
    "> * Hypothesis 2: \"The dog is running around in the garden.\" (Contradiction)\n",
    "> * Hypothesis 3: \"The dog is in the house.\" (Neutral)\n",
    "> \n",
    "> In hypothesis 1, it's entailed or directly implied by the premise. Hypothesis 2 contradicts the information given in the premise. Hypothesis 3 might be true based on the premise, but we don't have enough information to confirm, hence it's neutral.\n",
    "> \n",
    "> NLI is a crucial task for many NLP applications, such as machine translation, question answering, summarization, and more. The performance on NLI tasks is often seen as a measure of a system's ability to understand language in a meaningful and nuanced way."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Cleanup\n",
    " This Notebook has allocated resources that you may wish to tidy up."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.remove(parquet_filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.delete_index(pinecone_index_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Astra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.execute(f\"DROP TABLE {KEYSPACE_NAME}.{TABLE_NAME}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
